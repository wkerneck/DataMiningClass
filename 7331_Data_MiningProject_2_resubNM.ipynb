{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ![.right](https://lh6.ggpht.com/wUrTIhpHPVqw_VPCdhbSiF5TXlBlLyRHdH1gsII_y5NkgYKzbbj7cC2l6AGoVq-JN0U=w100 \"MSDS 7331 Data Mining - Project 1\")\n",
    "\n",
    "\n",
    "# Project 2: Classification\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Team Members\n",
    "\n",
    "- Chris Woodard\n",
    "- Claire Chu\n",
    "- Nathan Mowat\n",
    "- Bill Kerneckel\n",
    "\n",
    "---\n",
    "# Resubmission\n",
    "- Added in more thorough low false positive preference explanation\n",
    "- Added explanation for addition folds\n",
    "- Added Weighted-Coefficient plots to the Random Forest Classifiers\n",
    "- Removed salary variables from the salary_range_Jr_Level models to prevent 'data leakage'\n",
    "- Added Statistical Analysis\n",
    "    - One-way ANOVA with a post hoc test.\n",
    "\n",
    "# Rubic\n",
    "\n",
    "- [Data Preperation 1](#dp1) \n",
    "\n",
    "- [Data Preperation 2](#dp2)\n",
    "\n",
    "- [Modeling and Evaluation 1](#me1)\n",
    "\n",
    "- [Modeling and Evaluation 2](#me2)\n",
    "\n",
    "- [Modeling and Evaluation 3](#me3)\n",
    "\n",
    "- [Modeling and Evaluation 4](#me4)\n",
    "\n",
    "- [Modeling and Evaluation 5](#me5)\n",
    "\n",
    "- [Modeling and Evaluation 6](#me6)\n",
    "\n",
    "- [Deployment](#d)\n",
    "\n",
    "- [Exceptional Work](#ew)\n",
    "\n",
    "- [Appendix](#a)\n",
    "\n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OBJECTIVE: \n",
    "Using the NASA human resources dataset, let's use the variables to predict future records:\n",
    "- What kind of model can we use to predict a \"junior salary range level\" classification, given the variables we have?\n",
    "- What kind of model can we use to predict a \"caucasian\" race classification, given the variables we have?\n",
    "\n",
    "Based on these findings, can NASA Human Resources verify that salaries and positions are based on skill and experience, not race or gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Preperation 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">10 Points - Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load python libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import ggplot\n",
    "import datetime\n",
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load data file\n",
    "data_file = pd.read_excel('/Users/macnificent/Desktop/NEW_NASA_2006.xlsx') \n",
    "data_file2 = pd.read_excel('/Users/macnificent/Desktop/NEW_NASA_2007.xlsx') \n",
    "data_file3 = pd.read_excel('/Users/macnificent/Desktop/NASA_2008.xlsx')\n",
    "data_file4 = pd.read_excel('/Users/macnificent/Desktop/NASA_2009.xlsx')\n",
    "data_file5 = pd.read_excel('/Users/macnificent/Desktop/NASA_2010.xlsx')\n",
    "data_file6 = pd.read_excel('/Users/macnificent/Desktop/NASA_2011.xlsx')\n",
    "data_file7 = pd.read_excel('/Users/macnificent/Desktop/NASA_2012.xlsx')\n",
    "data_file8 = pd.read_excel('/Users/macnificent/Desktop/NASA_2013.xlsx')\n",
    "data_file9 = pd.read_excel('/Users/macnificent/Desktop/NASA_2014.xlsx')\n",
    "data_file10 = pd.read_excel('/Users/macnificent/Desktop/NASA_2015.xlsx')\n",
    "data_file11 = pd.read_excel('/Users/macnificent/Desktop/NASA_2016.xlsx')\n",
    "\n",
    "#you'll have to change the file path to your working directory\n",
    "\n",
    "#removing 1st row from datasets\n",
    "data_file.drop(0, axis = 0,inplace = True)\n",
    "data_file2.drop(0, axis = 0,inplace = True)\n",
    "data_file3.drop(0, axis = 0,inplace = True)\n",
    "data_file4.drop(0, axis = 0,inplace = True)\n",
    "data_file5.drop(0, axis = 0,inplace = True)\n",
    "data_file6.drop(0, axis = 0,inplace = True)\n",
    "data_file7.drop(0, axis = 0,inplace = True)\n",
    "data_file8.drop(0, axis = 0,inplace = True)\n",
    "data_file9.drop(0, axis = 0,inplace = True)\n",
    "data_file10.drop(0, axis = 0,inplace = True)\n",
    "data_file11.drop(0, axis = 0,inplace = True)\n",
    "\n",
    "#note: please be patient as this will take a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Hist_yr</th>\n",
       "      <th>Unnamed: 28</th>\n",
       "      <th>agency</th>\n",
       "      <th>bdyr</th>\n",
       "      <th>coopsch</th>\n",
       "      <th>coopyr</th>\n",
       "      <th>currgrddte</th>\n",
       "      <th>dtystn_ind</th>\n",
       "      <th>dtystnname</th>\n",
       "      <th>...</th>\n",
       "      <th>sex</th>\n",
       "      <th>step_emp</th>\n",
       "      <th>supind</th>\n",
       "      <th>suplev</th>\n",
       "      <th>tenure</th>\n",
       "      <th>time_in_grade</th>\n",
       "      <th>tl</th>\n",
       "      <th>ttl</th>\n",
       "      <th>typappt</th>\n",
       "      <th>worksch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/08/1989</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>205.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/22/2002</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN23</td>\n",
       "      <td>1986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/21/1996</td>\n",
       "      <td>88</td>\n",
       "      <td>HAMPTON,HAMPTON,VIRGINIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>124.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN64</td>\n",
       "      <td>1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/17/2005</td>\n",
       "      <td>88</td>\n",
       "      <td>STENNIS SPACE CENTER, HANCOCK, MISSISSIPPI</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>19.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/23/2006</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>3.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN62</td>\n",
       "      <td>1972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/01/1997</td>\n",
       "      <td>32</td>\n",
       "      <td>REDSTONE ARSENAL,MADISON,ALABAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>107.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/27/2000</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>80.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/26/1998</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>102.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/27/1987</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>229.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/14/2006</td>\n",
       "      <td>80</td>\n",
       "      <td>WASHINGTON,DISTRICT OF COLUMBIA</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>6.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN62</td>\n",
       "      <td>1982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/30/2000</td>\n",
       "      <td>32</td>\n",
       "      <td>REDSTONE ARSENAL,MADISON,ALABAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>75.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/29/1989</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>205.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN22</td>\n",
       "      <td>1973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/04/2004</td>\n",
       "      <td>14</td>\n",
       "      <td>BROOK PARK,CUYAHOGA,OHIO</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>31.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>1975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/30/1997</td>\n",
       "      <td>80</td>\n",
       "      <td>WASHINGTON,DISTRICT OF COLUMBIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>115.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/23/1984</td>\n",
       "      <td>88</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>262.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN21</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/14/2002</td>\n",
       "      <td>74</td>\n",
       "      <td>MOFFETT FIELD,SANTA CLARA,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>52.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN21</td>\n",
       "      <td>1979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/21/2004</td>\n",
       "      <td>74</td>\n",
       "      <td>MOFFETT FIELD,SANTA CLARA,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>32.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>2003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/27/2005</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>11.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>1978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/21/1993</td>\n",
       "      <td>41</td>\n",
       "      <td>PASADENA,LOS ANGELES,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>164.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN22</td>\n",
       "      <td>1983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/18/1990</td>\n",
       "      <td>14</td>\n",
       "      <td>BROOK PARK,CUYAHOGA,OHIO</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>192.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN62</td>\n",
       "      <td>1999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/15/2001</td>\n",
       "      <td>32</td>\n",
       "      <td>REDSTONE ARSENAL,MADISON,ALABAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>64.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/13/2005</td>\n",
       "      <td>88</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>12.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08/25/2002</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>50.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/15/2006</td>\n",
       "      <td>88</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/16/2006</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>7.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/08/1989</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>214.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>1972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/09/2006</td>\n",
       "      <td>80</td>\n",
       "      <td>WASHINGTON,DISTRICT OF COLUMBIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>10.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/12/2006</td>\n",
       "      <td>88</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/30/2005</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>12.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/12/2005</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>17.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202707</th>\n",
       "      <td>17498</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>08/05/2007</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>109.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AEROSPACE FLIGHT SYSTEMS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202708</th>\n",
       "      <td>17499</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>05/04/2014</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>28.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVISORY AST, SOFTWARE SYSTEMS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202709</th>\n",
       "      <td>17500</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN23</td>\n",
       "      <td>1991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>02/26/2012</td>\n",
       "      <td>88</td>\n",
       "      <td>HAMPTON,HAMPTON,VIRGINIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>54.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202710</th>\n",
       "      <td>17501</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>08/07/2016</td>\n",
       "      <td>88</td>\n",
       "      <td>BAY ST LOUIS,HANCOCK,MISSISSIPPI</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202711</th>\n",
       "      <td>17502</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN23</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10/02/2005</td>\n",
       "      <td>88</td>\n",
       "      <td>HAMPTON,HAMPTON,VIRGINIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>131.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202712</th>\n",
       "      <td>17503</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN23</td>\n",
       "      <td>2008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>07/12/2015</td>\n",
       "      <td>88</td>\n",
       "      <td>HAMPTON,HAMPTON,VIRGINIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>14.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARCH AST, AEROSPACE MATERIALS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202713</th>\n",
       "      <td>17504</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>07/24/1994</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>265.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>, TELECOMMUNICATIONS)</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202714</th>\n",
       "      <td>17505</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>03/11/2012</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>54.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202715</th>\n",
       "      <td>17506</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10/19/2014</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>23.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202716</th>\n",
       "      <td>17507</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>07/24/2016</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202717</th>\n",
       "      <td>17508</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>06/27/2004</td>\n",
       "      <td>54</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>146.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202718</th>\n",
       "      <td>17509</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN24</td>\n",
       "      <td>1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>06/12/2016</td>\n",
       "      <td>41</td>\n",
       "      <td>EDWARDS AFB,KERN,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202719</th>\n",
       "      <td>17510</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>05/05/2002</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>172.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202720</th>\n",
       "      <td>17511</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>03/28/2010</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>77.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>, AEROSPACE FLIGHT SYSTEMS)</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202721</th>\n",
       "      <td>17512</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN10</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10/11/2009</td>\n",
       "      <td>80</td>\n",
       "      <td>WASHINGTON,DISTRICT OF COLUMBIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>83.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202722</th>\n",
       "      <td>17513</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN21</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>03/13/2011</td>\n",
       "      <td>03</td>\n",
       "      <td>ALBUQUERQUE,BERNALILLO,NEW MEXICO</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>66.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202723</th>\n",
       "      <td>17514</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>09/08/1991</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>300.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>, AEROSPACE FLIGHT SYSTEMS)</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202724</th>\n",
       "      <td>17515</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>02/09/2014</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>31.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202725</th>\n",
       "      <td>17516</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN62</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11/05/2012</td>\n",
       "      <td>32</td>\n",
       "      <td>REDSTONE ARSENAL,MADISON,ALABAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>46.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARCH AST, FIELDS AND PARTICLES</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202726</th>\n",
       "      <td>17517</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>07/02/2000</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>194.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AEROSPACE FLIGHT SYSTEMS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202727</th>\n",
       "      <td>17518</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>2006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10/26/2009</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>82.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,ATMOSPHERIC CHEMISTRY &amp; DYNAMIC)</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202728</th>\n",
       "      <td>17519</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN72</td>\n",
       "      <td>1990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>04/03/2016</td>\n",
       "      <td>29</td>\n",
       "      <td>HOUSTON,HARRIS,TEXAS</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202729</th>\n",
       "      <td>17520</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN21</td>\n",
       "      <td>1985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>09/30/2007</td>\n",
       "      <td>74</td>\n",
       "      <td>MOFFETT FIELD,SANTA CLARA,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>107.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FLUID MECHANICS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202730</th>\n",
       "      <td>17521</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN62</td>\n",
       "      <td>2009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>09/19/2016</td>\n",
       "      <td>32</td>\n",
       "      <td>REDSTONE ARSENAL,MADISON,ALABAMA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARCH AST, FIELDS AND PARTICLES</td>\n",
       "      <td>15</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202731</th>\n",
       "      <td>17522</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>01/30/2000</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>199.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AEROSPACE FLIGHT SYSTEMS</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202732</th>\n",
       "      <td>17523</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN21</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10/29/2006</td>\n",
       "      <td>74</td>\n",
       "      <td>MOFFETT FIELD,SANTA CLARA,CALIFORNIA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>118.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202733</th>\n",
       "      <td>17524</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN76</td>\n",
       "      <td>1987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>01/11/2004</td>\n",
       "      <td>54</td>\n",
       "      <td>KENNEDY SPACE CENTER,BREVARD,FLORIDA</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>152.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>, SAFETY &amp; MISSION ASSURANCE)</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202734</th>\n",
       "      <td>17525</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NN51</td>\n",
       "      <td>1984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>08/26/2001</td>\n",
       "      <td>80</td>\n",
       "      <td>GREENBELT,PRINCE GEORGE'S,MARYLAND</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>180.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202735</th>\n",
       "      <td>17526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202736</th>\n",
       "      <td>17527</td>\n",
       "      <td>(17525 row(s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>) affec</td>\n",
       "      <td>ted)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202737 rows  57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index       Hist_yr Unnamed: 28   agency  bdyr coopsch coopyr  \\\n",
       "0           1          2006         NaN     NN51  1900     NaN    NaN   \n",
       "1           2          2006         NaN     NN51  1980     NaN    NaN   \n",
       "2           3          2006         NaN     NN23  1986     NaN    NaN   \n",
       "3           4          2006         NaN     NN64  1994     NaN    NaN   \n",
       "4           5          2006         NaN     NN72  2006     NaN    NaN   \n",
       "5           6          2006         NaN     NN62  1972     NaN    NaN   \n",
       "6           7          2006         NaN     NN51  1900     NaN    NaN   \n",
       "7           8          2006         NaN     NN72  1986     NaN    NaN   \n",
       "8           9          2006         NaN     NN51  1978     NaN    NaN   \n",
       "9          10          2006         NaN     NN10  1987     NaN    NaN   \n",
       "10         11          2006         NaN     NN62  1982     NaN    NaN   \n",
       "11         12          2006         NaN     NN72  1976     NaN    NaN   \n",
       "12         13          2006         NaN     NN22  1973     NaN    NaN   \n",
       "13         14          2006         NaN     NN10  1975     NaN    NaN   \n",
       "14         15          2006         NaN     NN76  1978     NaN    NaN   \n",
       "15         16          2006         NaN     NN21  1980     NaN    NaN   \n",
       "16         17          2006         NaN     NN21  1979     NaN    NaN   \n",
       "17         18          2006         NaN     NN51  2003     NaN    NaN   \n",
       "18         19          2006         NaN     NN10  1978     NaN    NaN   \n",
       "19         20          2006         NaN     NN22  1983     NaN    NaN   \n",
       "20         21          2006         NaN     NN62  1999     NaN    NaN   \n",
       "21         22          2006         NaN     NN76  1900     NaN    NaN   \n",
       "22         23          2006         NaN     NN51  1987     NaN    NaN   \n",
       "23         24          2006         NaN     NN76  1995     NaN    NaN   \n",
       "24         25          2006         NaN     NN51  2002     NaN    NaN   \n",
       "25         26          2006         NaN     NN72  1967     NaN    NaN   \n",
       "26         27          2006         NaN     NN10  1972     NaN    NaN   \n",
       "27         28          2006         NaN     NN76  1982     NaN    NaN   \n",
       "28         29          2006         NaN     NN51  1900     NaN    NaN   \n",
       "29         30          2006         NaN     NN72  1996     NaN    NaN   \n",
       "...       ...           ...         ...      ...   ...     ...    ...   \n",
       "202707  17498          2016         NaN     NN51  1992     NaN      0   \n",
       "202708  17499          2016         NaN     NN51  1992     NaN      0   \n",
       "202709  17500          2016         NaN     NN23  1991     NaN      0   \n",
       "202710  17501          2016         NaN     NN10  2005     NaN      0   \n",
       "202711  17502          2016         NaN     NN23  1900     NaN      0   \n",
       "202712  17503          2016         NaN     NN23  2008     NaN      0   \n",
       "202713  17504          2016         NaN     NN72  1987     NaN      0   \n",
       "202714  17505          2016         NaN     NN51  1997     NaN      0   \n",
       "202715  17506          2016         NaN     NN51  1900     NaN      0   \n",
       "202716  17507          2016         NaN     NN51  1900     NaN      0   \n",
       "202717  17508          2016         NaN     NN76  1997     NaN      0   \n",
       "202718  17509          2016         NaN     NN24  1997     NaN      0   \n",
       "202719  17510          2016         NaN     NN51  1983     NaN      0   \n",
       "202720  17511          2016         NaN     NN51  1991     NaN      0   \n",
       "202721  17512          2016         NaN     NN10  1987     NaN      0   \n",
       "202722  17513          2016         NaN     NN21  1900     NaN      0   \n",
       "202723  17514          2016         NaN     NN72  2013     NaN      0   \n",
       "202724  17515          2016         NaN     NN51  1989     NaN      0   \n",
       "202725  17516          2016         NaN     NN62  2010     NaN      0   \n",
       "202726  17517          2016         NaN     NN51  1987     NaN      0   \n",
       "202727  17518          2016         NaN     NN51  2006     NaN      0   \n",
       "202728  17519          2016         NaN     NN72  1990     NaN      0   \n",
       "202729  17520          2016         NaN     NN21  1985     NaN      0   \n",
       "202730  17521          2016         NaN     NN62  2009     NaN      0   \n",
       "202731  17522          2016         NaN     NN51  1974     NaN      0   \n",
       "202732  17523          2016         NaN     NN21  1980     NaN      0   \n",
       "202733  17524          2016         NaN     NN76  1987     NaN      0   \n",
       "202734  17525          2016         NaN     NN51  1984     NaN      0   \n",
       "202735  17526           NaN         NaN      NaN   NaN     NaN    NaN   \n",
       "202736  17527  (17525 row(s         NaN  ) affec  ted)     NaN    NaN   \n",
       "\n",
       "        currgrddte dtystn_ind                                  dtystnname  \\\n",
       "0       10/08/1989         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "1       09/22/2002         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "2       07/21/1996         88                    HAMPTON,HAMPTON,VIRGINIA   \n",
       "3       04/17/2005         88  STENNIS SPACE CENTER, HANCOCK, MISSISSIPPI   \n",
       "4       07/23/2006         29                        HOUSTON,HARRIS,TEXAS   \n",
       "5       12/01/1997         32            REDSTONE ARSENAL,MADISON,ALABAMA   \n",
       "6       02/27/2000         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "7       04/26/1998         29                        HOUSTON,HARRIS,TEXAS   \n",
       "8       09/27/1987         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "9       05/14/2006         80             WASHINGTON,DISTRICT OF COLUMBIA   \n",
       "10      07/30/2000         32            REDSTONE ARSENAL,MADISON,ALABAMA   \n",
       "11      09/29/1989         29                        HOUSTON,HARRIS,TEXAS   \n",
       "12      04/04/2004         14                    BROOK PARK,CUYAHOGA,OHIO   \n",
       "13      03/30/1997         80             WASHINGTON,DISTRICT OF COLUMBIA   \n",
       "14      12/23/1984         88        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "15      07/14/2002         74        MOFFETT FIELD,SANTA CLARA,CALIFORNIA   \n",
       "16      03/21/2004         74        MOFFETT FIELD,SANTA CLARA,CALIFORNIA   \n",
       "17      11/27/2005         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "18      03/21/1993         41             PASADENA,LOS ANGELES,CALIFORNIA   \n",
       "19      11/18/1990         14                    BROOK PARK,CUYAHOGA,OHIO   \n",
       "20      07/15/2001         32            REDSTONE ARSENAL,MADISON,ALABAMA   \n",
       "21      11/13/2005         88        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "22      08/25/2002         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "23      10/15/2006         88        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "24      04/16/2006         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "25      01/08/1989         29                        HOUSTON,HARRIS,TEXAS   \n",
       "26      01/09/2006         80             WASHINGTON,DISTRICT OF COLUMBIA   \n",
       "27      11/12/2006         88        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "28      10/30/2005         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "29      06/12/2005         29                        HOUSTON,HARRIS,TEXAS   \n",
       "...            ...        ...                                         ...   \n",
       "202707  08/05/2007         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202708  05/04/2014         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202709  02/26/2012         88                    HAMPTON,HAMPTON,VIRGINIA   \n",
       "202710  08/07/2016         88            BAY ST LOUIS,HANCOCK,MISSISSIPPI   \n",
       "202711  10/02/2005         88                    HAMPTON,HAMPTON,VIRGINIA   \n",
       "202712  07/12/2015         88                    HAMPTON,HAMPTON,VIRGINIA   \n",
       "202713  07/24/1994         29                        HOUSTON,HARRIS,TEXAS   \n",
       "202714  03/11/2012         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202715  10/19/2014         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202716  07/24/2016         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202717  06/27/2004         54        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "202718  06/12/2016         41                 EDWARDS AFB,KERN,CALIFORNIA   \n",
       "202719  05/05/2002         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202720  03/28/2010         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202721  10/11/2009         80             WASHINGTON,DISTRICT OF COLUMBIA   \n",
       "202722  03/13/2011         03           ALBUQUERQUE,BERNALILLO,NEW MEXICO   \n",
       "202723  09/08/1991         29                        HOUSTON,HARRIS,TEXAS   \n",
       "202724  02/09/2014         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202725  11/05/2012         32            REDSTONE ARSENAL,MADISON,ALABAMA   \n",
       "202726  07/02/2000         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202727  10/26/2009         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202728  04/03/2016         29                        HOUSTON,HARRIS,TEXAS   \n",
       "202729  09/30/2007         74        MOFFETT FIELD,SANTA CLARA,CALIFORNIA   \n",
       "202730  09/19/2016         32            REDSTONE ARSENAL,MADISON,ALABAMA   \n",
       "202731  01/30/2000         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202732  10/29/2006         74        MOFFETT FIELD,SANTA CLARA,CALIFORNIA   \n",
       "202733  01/11/2004         54        KENNEDY SPACE CENTER,BREVARD,FLORIDA   \n",
       "202734  08/26/2001         80          GREENBELT,PRINCE GEORGE'S,MARYLAND   \n",
       "202735         NaN        NaN                                         NaN   \n",
       "202736         NaN        NaN                                         NaN   \n",
       "\n",
       "         ...    sex step_emp supind suplev tenure time_in_grade   tl  \\\n",
       "0        ...      F        9      8     34      1        205.46  NaN   \n",
       "1        ...      F        6      2     26      1         50.00  NaN   \n",
       "2        ...      M        8      8     34      1        124.04  NaN   \n",
       "3        ...      M        2      2     26      1         19.17  NaN   \n",
       "4        ...      F       10      8     34      3          3.98  NaN   \n",
       "5        ...      M        6      8     34      1        107.69  NaN   \n",
       "6        ...      M        5      8     34      1         80.85  NaN   \n",
       "7        ...      M        6      8     34      1        102.88  NaN   \n",
       "8        ...      M        0      2     32      1        229.85  NaN   \n",
       "9        ...      F        6      2     34      1          6.27  NaN   \n",
       "10       ...      M        7      8     34      1         75.75  NaN   \n",
       "11       ...      F       10      8     34      1        205.78  NaN   \n",
       "12       ...      M        7      2     29      1         31.59  NaN   \n",
       "13       ...      M        8      5     34      1        115.75  NaN   \n",
       "14       ...      F       10      5     25      1        262.98  NaN   \n",
       "15       ...      M        5      8     34      1         52.27  NaN   \n",
       "16       ...      M       10      5     34      1         32.04  NaN   \n",
       "17       ...      M        4      8     34      1         11.85  NaN   \n",
       "18       ...      M        8      8     34      1        164.04  NaN   \n",
       "19       ...      M       10      2     27      1        192.14  NaN   \n",
       "20       ...      M        8      8     34      1         64.24  NaN   \n",
       "21       ...      F       10      8     34      3         12.30  NaN   \n",
       "22       ...      M        7      8     34      1         50.91  NaN   \n",
       "23       ...      M        1      8     34      3          1.24  NaN   \n",
       "24       ...      F        2      8     34      1          7.20  NaN   \n",
       "25       ...      M       10      8     34      1        214.46  NaN   \n",
       "26       ...      M        0      2     26      0         10.43  NaN   \n",
       "27       ...      M       10      8     34      3          0.33  NaN   \n",
       "28       ...      F        2      8     34      1         12.75  NaN   \n",
       "29       ...      M        8      8     34      2         17.33  NaN   \n",
       "...      ...    ...      ...    ...    ...    ...           ...  ...   \n",
       "202707   ...      F        8      8     34      1        109.59  NaN   \n",
       "202708   ...      M        7      2     29      1         28.62  NaN   \n",
       "202709   ...      M        5      8     34      1         54.91  NaN   \n",
       "202710   ...      M        6      8     32      1          1.52  NaN   \n",
       "202711   ...      M        7      2     29      1        131.68  NaN   \n",
       "202712   ...      M        3      8     34      1         14.36  NaN   \n",
       "202713   ...      F       10      8     34      1        265.98  NaN   \n",
       "202714   ...      F        5      8     34      1         54.39  NaN   \n",
       "202715   ...      F        4      8     34      1         23.14  NaN   \n",
       "202716   ...      F        5      2     27      1          1.98  NaN   \n",
       "202717   ...      F        8      8     34      1        146.88  NaN   \n",
       "202718   ...      F        5      8     34      1          3.36  NaN   \n",
       "202719   ...      M       10      8     34      1        172.59  NaN   \n",
       "202720   ...      M        7      8     34      1         77.85  NaN   \n",
       "202721   ...      M        0      2     26      0         83.39  NaN   \n",
       "202722   ...      M        8      8     34      1         66.33  NaN   \n",
       "202723   ...      M       10      8     34      1        300.49  NaN   \n",
       "202724   ...      F        5      5     29      1         31.46  NaN   \n",
       "202725   ...      M        4      8     34      1         46.59  NaN   \n",
       "202726   ...      M        9      8     34      1        194.68  NaN   \n",
       "202727   ...      M        5      8     34      1         82.91  NaN   \n",
       "202728   ...      F        4      8     34      1          5.65  NaN   \n",
       "202729   ...      M        9      8     34      1        107.78  NaN   \n",
       "202730   ...      M        1      8     34      2          0.14  NaN   \n",
       "202731   ...      M       10      8     34      1        199.78  NaN   \n",
       "202732   ...      M        8      8     34      1        118.81  NaN   \n",
       "202733   ...      M       10      8     34      1        152.39  NaN   \n",
       "202734   ...      M        0      8     34      1        180.91  NaN   \n",
       "202735   ...    NaN      NaN    NaN    NaN    NaN           NaN  NaN   \n",
       "202736   ...    NaN      NaN    NaN    NaN    NaN           NaN  NaN   \n",
       "\n",
       "                                      ttl typappt worksch  \n",
       "0                                     NaN      10       F  \n",
       "1                                     NaN      10       F  \n",
       "2                                     NaN      10       F  \n",
       "3                                     NaN      10       F  \n",
       "4                                     NaN      20       F  \n",
       "5                                     NaN      10       F  \n",
       "6                                     NaN      10       F  \n",
       "7                                     NaN      10       F  \n",
       "8                                     NaN      10       F  \n",
       "9                                     NaN      10       F  \n",
       "10                                    NaN      10       F  \n",
       "11                                    NaN      10       F  \n",
       "12                                    NaN      10       F  \n",
       "13                                    NaN      10       F  \n",
       "14                                    NaN      10       F  \n",
       "15                                    NaN      10       F  \n",
       "16                                    NaN      10       F  \n",
       "17                                    NaN      10       F  \n",
       "18                                    NaN      10       F  \n",
       "19                                    NaN      10       F  \n",
       "20                                    NaN      10       F  \n",
       "21                                    NaN      20       F  \n",
       "22                                    NaN      10       F  \n",
       "23                                    NaN      20       F  \n",
       "24                                    NaN      10       F  \n",
       "25                                    NaN      10       F  \n",
       "26                                    NaN      50       F  \n",
       "27                                    NaN      20       F  \n",
       "28                                    NaN      10       F  \n",
       "29                                    NaN      15       F  \n",
       "...                                   ...     ...     ...  \n",
       "202707           AEROSPACE FLIGHT SYSTEMS      10       F  \n",
       "202708      RVISORY AST, SOFTWARE SYSTEMS      10       F  \n",
       "202709                                NaN      10       F  \n",
       "202710                                NaN      10       F  \n",
       "202711                                NaN      10       F  \n",
       "202712      ARCH AST, AEROSPACE MATERIALS      10       F  \n",
       "202713              , TELECOMMUNICATIONS)      10       F  \n",
       "202714                                NaN      10       F  \n",
       "202715                                NaN      10       F  \n",
       "202716                                NaN      10       F  \n",
       "202717                                NaN      10       F  \n",
       "202718                                NaN      10       F  \n",
       "202719                                NaN      10       F  \n",
       "202720        , AEROSPACE FLIGHT SYSTEMS)      10       F  \n",
       "202721                                NaN      50       F  \n",
       "202722                                NaN      10       F  \n",
       "202723        , AEROSPACE FLIGHT SYSTEMS)      10       F  \n",
       "202724                                NaN      10       F  \n",
       "202725     ARCH AST, FIELDS AND PARTICLES      10       F  \n",
       "202726           AEROSPACE FLIGHT SYSTEMS      10       F  \n",
       "202727  ,ATMOSPHERIC CHEMISTRY & DYNAMIC)      10       F  \n",
       "202728                                NaN      10       F  \n",
       "202729                    FLUID MECHANICS      10       F  \n",
       "202730     ARCH AST, FIELDS AND PARTICLES      15       F  \n",
       "202731           AEROSPACE FLIGHT SYSTEMS      10       F  \n",
       "202732                                NaN      10       F  \n",
       "202733      , SAFETY & MISSION ASSURANCE)      10       F  \n",
       "202734                                NaN      10       F  \n",
       "202735                                NaN     NaN     NaN  \n",
       "202736                                NaN     NaN     NaN  \n",
       "\n",
       "[202737 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining dataframes \n",
    "df = [data_file, data_file2, data_file3, data_file4, data_file5, data_file6, data_file7, data_file8, data_file9, data_file10, data_file11]\n",
    "\n",
    "cdf = pd.concat(df, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "cdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dropping columns we are not using for the analysis\n",
    "cdf.drop('Unnamed: 28', axis=1, inplace=True)#############################\n",
    "cdf.drop('agency', axis=1, inplace=True)##################################\n",
    "cdf.drop('coopsch', axis=1, inplace=True)\n",
    "cdf.drop('coopyr', axis=1, inplace=True)\n",
    "cdf.drop('dtystn_ind', axis=1, inplace=True)\n",
    "cdf.drop('dtystnname', axis=1, inplace=True)\n",
    "cdf.drop('edlev', axis=1, inplace=True)\n",
    "cdf.drop('foulev', axis=1, inplace=True)\n",
    "cdf.drop('fousch', axis=1, inplace=True)\n",
    "cdf.drop('fouyr', axis=1, inplace=True)\n",
    "cdf.drop('nasattl', axis=1, inplace=True)\n",
    "cdf.drop('nasa', axis=1, inplace=True)#########################################\n",
    "cdf.drop('nasat', axis=1, inplace=True)########################################\n",
    "cdf.drop('probenddte', axis=1, inplace=True)\n",
    "cdf.drop('promontedte', axis=1, inplace=True)\n",
    "cdf.drop('secsch', axis=1, inplace=True)\n",
    "cdf.drop('seclev', axis=1, inplace=True)\n",
    "cdf.drop('typappt', axis=1, inplace=True)\n",
    "cdf.drop('tl', axis=1, inplace=True)###########################################\n",
    "cdf.drop('ttl', axis=1, inplace=True)##########################################\n",
    "cdf.drop('worksch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cdf = cdf[cdf.bdyr != 1900]\n",
    "cdf.salary = cdf.salary.astype(np.float64)\n",
    "cdf.time_in_grade = cdf.time_in_grade.astype(np.float64)\n",
    "cdf.suplev = cdf.suplev.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# converting bdyr column to age\n",
    "#now = datetime.datetime.now()\n",
    "now = date(2016,12,31)\n",
    "cdf['bdyr'] = cdf['bdyr'].apply(pd.to_numeric, errors='coerce') \n",
    "cdf['age'] = now.year - cdf['bdyr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     Jr_Level\n",
      "2    Mid_Level\n",
      "3     Jr_Level\n",
      "4    Mid_Level\n",
      "5     Jr_Level\n",
      "Name: salary_range, dtype: category\n",
      "Categories (4, object): [Entry_Level < Jr_Level < Mid_Level < Mgmt_Level]\n"
     ]
    }
   ],
   "source": [
    "cdf['salary_range'] = pd.cut(cdf['salary'],[0,50000,100000,135000,150000],4,labels=['Entry_Level','Jr_Level','Mid_Level', 'Mgmt_Level'])\n",
    "print(cdf['salary_range'].head())\n",
    "cdf.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149993.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.salary.max() #Is the salary range cut-off still valid with the new data? Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# convert cdf variable back to \"df\" nomenclature\n",
    "df = pd.DataFrame(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#manipulate employee start date 'eoddte' to get 'service' variable\n",
    "#convert 'eoddte' to series\n",
    "df.eod = pd.Series(df['eoddte'])\n",
    "#convert 'eoddte' series to 'eoddte' datetime\n",
    "df['eoddte'] = pd.to_datetime(df.eod) \n",
    "#convert eod to be just the year\n",
    "df['eodyr'] = df['eoddte'].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#convert high school graduation year \"hiyr\" to be numeric\n",
    "df['hiyr'] = df['hiyr'].apply(pd.to_numeric, errors='coerce')\n",
    "#after looking at a crosstab, we can see that there are a lot of \"high school graduation year: 0\"\n",
    "#this is a data entry error, let's remove these entries...\n",
    "df = df[df.hiyr != 0]\n",
    "pd.crosstab(index=df[\"hiyr\"],columns=\"hiyr\")\n",
    "#now the 0 entries have been removed\n",
    "\n",
    "#subtract employee hire date from high school graduation date to get exprience\n",
    "df['experience'] = df['eodyr'] - df['hiyr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get the 'service' variable by subtracting current date from hire date\n",
    "today = date(2016, 12, 31)\n",
    "\n",
    "df['service'] = today - df['eoddte']\n",
    "#generate retirement potential by subtracting retirement eligibility date from todays date  \n",
    "#convert retoptdte to series for conversion to datetime format\n",
    "df.retoptdte = pd.Series(df['retoptdte'])\n",
    "#convert 'retoptdte' series to 'eoddte' datetime\n",
    "df['retoptdte'] = pd.to_datetime(df.retoptdte) \n",
    "#generate 'retpot' variable\n",
    "df['retpot'] = today - df['retoptdte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(by=['grade','sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(by=['grade','sex'])\n",
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "df_imputed[['grade','sex']] = df[['grade','sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['Hist_yr'] = df['Hist_yr'].astype(np.float64)\n",
    "df_yrgroup = df.groupby(by=['Hist_yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# creating new dataframe. Keeping original seperate.\n",
    "DF_NoT = df.copy()\n",
    "# perform one-hot encoding of the categorical data \"salary_range\" and \"rno\" (ethnic background).\n",
    "tmp_df = pd.get_dummies(df.salary_range,prefix='salary_range')\n",
    "tmp_df2 = pd.get_dummies(df.rno,prefix='rno')\n",
    "\n",
    "DF_NoT = pd.concat((df,tmp_df),axis=1) # add back into the dataframe\n",
    "DF_NoT = pd.concat((DF_NoT,tmp_df2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Delete time/dates - can't be used in L-Regression\n",
    "DF_NoT = DF_NoT.drop(['eoddte','retoptdte','salary_range', 'currgrddte', 'frscdte', 'fscdte', 'hisch', 'leqdte',\n",
    "                     'loccde','ncc', 'opmtitle', 'occode', 'orga', 'orgabr', 'orgadir', 'orgadiv',  'orgasec', \n",
    "                     'postenure', 'tenure', 'supind', 'suplev', 'rno', 'rno_000010', 'rno_100100', 'rno_100001', \n",
    "                     'rno_100010', 'rno_100101', 'rno_100000', 'rno_011000', 'rno_001101', 'rno_010001', 'rno_010101',\n",
    "                     'rno_001100', 'rno_010000', 'rno_010100', 'rno_001001', 'rno_001000', 'rno_000101', 'rno_000100',\n",
    "                     'rno_000011', 'rno_101000', 'rno_101101', 'rno_110000', 'rno_110001', 'rno_110101', 'rno_111111',\n",
    "                     'rno_10', 'rno_11', 'rno_100', 'rno_101', 'rno_1000', 'rno_1001', 'rno_1100', 'rno_1101', 'rno_10000',\n",
    "                     'rno_10001', 'rno_10100', 'rno_10101', 'rno_101100', 'rno_000000', 'rno_000110', 'rno_000111', \n",
    "                     'rno_001010', 'rno_001011', 'rno_010111', 'rno_011001', 'rno_011100', 'rno_011101', 'rno_100011', \n",
    "                      'rno_101001', 'rno_1'\n",
    "                     ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "today = date(2016, 12, 31) #modify if more datasets are added\n",
    "DF_NoT.service = DF_NoT.service / np.timedelta64(1, 'D')\n",
    "DF_NoT.retpot = DF_NoT.retpot / np.timedelta64(1, 'D')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF_NoT.lastpromodte = pd.Series(DF_NoT['lastpromodte'])\n",
    "#convert 'eoddte' series to 'eoddte' datetime\n",
    "DF_NoT['lastpromodte'] = pd.to_datetime(DF_NoT.lastpromodte) \n",
    "DF_NoT['lastpromodte'] = today - DF_NoT['lastpromodte']\n",
    "\n",
    "#removing the day to calculate a number\n",
    "DF_NoT.lastpromodte = DF_NoT.lastpromodte / np.timedelta64(1, 'D')\n",
    "\n",
    "DF_NoT.nextwigdte = pd.Series(DF_NoT['nextwigdte'])\n",
    "DF_NoT['nextwigdte'] = pd.to_datetime(DF_NoT.nextwigdte) \n",
    "DF_NoT['nextwigdte'] = today - DF_NoT['nextwigdte']\n",
    "DF_NoT.nextwigdte = DF_NoT.nextwigdte / np.timedelta64(1, 'D')\n",
    "\n",
    "DF_NoT.retdiscdte = pd.Series(DF_NoT['retdiscdte'])\n",
    "DF_NoT['retdiscdte'] = pd.to_datetime(DF_NoT.retdiscdte) \n",
    "DF_NoT['retdiscdte'] = today - DF_NoT['retdiscdte']\n",
    "DF_NoT.retdiscdte = DF_NoT.retdiscdte / np.timedelta64(1, 'D')\n",
    "\n",
    "DF_NoT['IsMale'] = DF_NoT.sex=='M' \n",
    "DF_NoT.IsMale = DF_NoT.IsMale.astype(np.int)\n",
    "del DF_NoT['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF_Reg2 = DF_NoT.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Preperation 2\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# RESUBMISSION PIECE - REMOVED SALARY VARIABLES\n",
    "del DF_Reg2['salary']\n",
    "del DF_Reg2['salary_range_Entry_Level']\n",
    "del DF_Reg2['salary_range_Mid_Level']\n",
    "del DF_Reg2['salary_range_Mgmt_Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">5 Points - Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 126967 entries, 2 to 17523\n",
      "Data columns (total 20 columns):\n",
      "Hist_yr                  126967 non-null float64\n",
      "bdyr                     126967 non-null float64\n",
      "grade                    126967 non-null object\n",
      "hilev                    126967 non-null object\n",
      "hiyr                     126967 non-null int64\n",
      "install                  126967 non-null object\n",
      "lastpromodte             126967 non-null float64\n",
      "nextwigdte               126967 non-null float64\n",
      "retdiscdte               126967 non-null float64\n",
      "secyr                    126967 non-null object\n",
      "step_emp                 126967 non-null object\n",
      "time_in_grade            126967 non-null float64\n",
      "age                      126967 non-null float64\n",
      "eodyr                    126967 non-null int64\n",
      "experience               126967 non-null int64\n",
      "service                  126967 non-null float64\n",
      "retpot                   126967 non-null float64\n",
      "salary_range_Jr_Level    126967 non-null uint8\n",
      "rno_000001               126967 non-null uint8\n",
      "IsMale                   126967 non-null int64\n",
      "dtypes: float64(9), int64(4), object(5), uint8(2)\n",
      "memory usage: 18.6+ MB\n"
     ]
    }
   ],
   "source": [
    "DF_Reg2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the output above, you can see we have about 126,967 entries with 20 columns. \n",
    "New Variables created are:\n",
    "- the age category derived from the \"birth year\" variable\n",
    "- the salary_range category derived by dividing up the salary variable into 4 categories\n",
    "- the salary_range_entry_level is a dummy variable established form the salary range category\n",
    "- the salary_range_jr_level is a dummy variable established form the salary range category\n",
    "- the salary_range_mid_level is a dummy variable established form the salary range category\n",
    "- the salary_range_mgmt_level is a dummy variable established form the salary range category\n",
    "- the eodyr variable which shows the year of employment\n",
    "- the experience variable derived from the high school graduation year subtracted from the last date of the data set (12/31/2016)\n",
    "- the serivce variable derived from the year of employment subtracted from 12/31/2016\n",
    "- the retirement potential derived from the date of earliest reitrement subtracted from 12/31/2016\n",
    "- the rno_000001 variable is derived from the RNO or diversity classification category where 000001 represents the Caucasian Identity Classification\n",
    "- the IsMale variable is a dummy variable established from the gender classification variable.\n",
    "\n",
    "Let's use the variables to predict future records in the NASA human resources data set\n",
    "- which model can we use to predict a \"junior salary range level\" classification, given the variables we have\n",
    "- which model can we use to predict a \"caucasian\" race classification, given the variables we have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 1: KNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">10 Points - Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We utilized ACCURACY as our evaluation metric of choice because we want to classify a salary range and the diversity (race/national origin) category.  A low false positive score is ideal because we do not want to incorrectly classify the data points.  All things being equal, it would be better if we had more false negatives. For this dataset, logic steers us to be accurate overall since we have four classes of salary range and multiple classes for the diversity classification.  \n",
    "\n",
    "An argument can be made for looking at false negatives on a per class basis. For instance, after reviewing our initial data exploration, we found that the salary range class was unevenly distributed, so we might observe a low count of false positives. This makes sense since a person in a salary range with relatively few measured data points may trigger a low amount of false negatives under some models. Low false positives (and conversely, high true positives) indicate that our model is predicting that the observation is \"true\" when it is, in fact \"true\". Why focus on low false positives and not low false negatives?\n",
    "\n",
    "For the sake of brevity in this assignment, our models will attempt to predict two things: whether an observation is in the junior level salary range or not and whether an observation is in the caucasian diversity class or not. Since there are another 3 salary range levels and another 19 diversity classes, a classification of \"not junior level\" or \"not caucasian\" is still pretty vague. Therefore, we want to make sure our results have a high true positive score. \n",
    "\n",
    "In the initial data collection stages, we also noticed that the RNO classifiers are not only exclusive categories (Caucasian, Mexican, Asian, African-American, etc.) but also combinations of categories (one employee can identify as a mix of Caucasian, Mexican, and Asian for example). Because these combinations are listed under different RNO categories (rno_000110 instead of rno_000001, rno_001010, and rno_000111), even though an employee identifies as partially caucasian, they are not counted in the caucasian category. For this reason, we expect an amount of entries that will classify as false negative because they are a combination of backgrounds and will not show up as exclusively caucasian.\n",
    "\n",
    "Since, we anticipate that this modeling and dataset be used to verify that NASA HR has practices that are consistent with promoting employees based on skill and experience (vs. gender, race, etc.). We expect that if the model shows that gender and race are determining factors in our final model for salary range, that HR can use this as a tool to re-examine current policy. Similarly, we expect that if salary and grade are determining factors for the caucasian diversity classifier that this is another red flag that should indicate an error in currect policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN junior salary range accuracy 0.783148377137\n",
      "[[70105 11585]\n",
      " [15948 29329]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# X = create a matrix with all variable except response variable in it. rno_000001\n",
    "# Y = this should be predictor variable\n",
    "# create variables we are more familiar with\n",
    "DF_NoT_KNN1 = DF_Reg2.copy()\n",
    "del DF_NoT_KNN1['salary_range_Jr_Level']\n",
    "\n",
    "X = DF_NoT_KNN1.as_matrix()\n",
    "y = DF_Reg2['salary_range_Jr_Level'].as_matrix()\n",
    "\n",
    "\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "KNN1_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    KNN1_clf.fit(X[train],y[train])\n",
    "    yhat[test] = KNN1_clf.predict(X[test])\n",
    "\n",
    "KNNs_total_accuracy = mt.accuracy_score(y, yhat)\n",
    "KNNs_conf = mt.confusion_matrix(y, yhat)\n",
    "print ('KNN junior salary range accuracy', KNNs_total_accuracy)\n",
    "print(KNNs_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the KNN accuracy rating for 'Junior Salary Range Level', we can compare the KNN model to other models. The accuracy score is 78% so it will be interesting to see if any of the other models can beat it.\n",
    "\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "\n",
    "true positives (TP) 29329 entries: These are cases in which we predicted yes (they are in the Junior Salary Range), and they are in the Junior Salary Range.\n",
    "\n",
    "true negatives (TN) 70105 entries: We predicted no, and they were not in the Junior Salary Range.\n",
    "\n",
    "false positives (FP or Type I Error) 11585 entries: We predicted yes, but they were not actually in the junior Salary Range.\n",
    "\n",
    "false negatives (FN or Type II Error) 15948 entries: We predicted no, but they actually are in the Junior Salary Range. \n",
    "\n",
    "EXPLAINING THE CONFUSION MATRIX: \n",
    "we have 29329 as the true positive and 70105 as the true negative. This makes sense since our data showed that Junior Salary Range entries comprised of only about 25% of the dataset (with the other entries being, entry_level, mid_level, and mgmt_level).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Adversity Accuracy 0.552151346413\n",
      "[[17548 24319]\n",
      " [32543 52557]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# X = create a matrix with all variable except response variable in it. rno_000001\n",
    "# Y = this should be predictor variable\n",
    "# create variables we are more familiar with\n",
    "DF_NoT_KNN2 = DF_NoT.copy()\n",
    "del DF_NoT_KNN2['rno_000001']\n",
    "\n",
    "X2 = DF_NoT_KNN2.as_matrix()\n",
    "y2 = DF_NoT['rno_000001'].as_matrix()\n",
    "\n",
    "\n",
    "yhat2 = np.zeros(y2.shape) # we will fill this with predictions\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X2,y2):\n",
    "    clf.fit(X2[train],y2[train])\n",
    "    yhat2[test] = clf.predict(X2[test])\n",
    "\n",
    "KNNr_total_accuracy2 = mt.accuracy_score(y2, yhat2)\n",
    "KNNr_conf2 = mt.confusion_matrix(y2, yhat2)\n",
    "print ('KNN Adversity Accuracy', KNNr_total_accuracy2)\n",
    "print(KNNr_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the KNN accuracy rating for 'rno_00001' or the adveristy classifier, we can compare the KNN model to other models. The accuracy score is about 55% so let's see if any other models can provide better results. \n",
    "Compared to the Salary Range Accuracy, the Adversity Accuracy score is not that good. However, this makes sense since NASA works hard to maintain a diverse workforce both across departments and across management levels.\n",
    "\n",
    "http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "\n",
    "true positives (TP) 52557 entries: These are cases in which we predicted yes (they are caucasian), and they employee was in fact caucasian.\n",
    "\n",
    "true negatives (TN) 17548 entries: We predicted no, and they were not caucasian.\n",
    "\n",
    "false positives (FP or Type I Error) 24319 entries: We predicted yes, but they were not caucasian.\n",
    "\n",
    "false negatives (FN or Type II Error): We predicted no, but they actually do have the disease. \n",
    "\n",
    "EXPLAINING THE CONFUSION MATRIX: \n",
    "We have 52557 entries for the true positives and 13548 entries for the true negatives. This makes sense since a majority of the workforce at NASA is caucasian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">10 Points - Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After running the previous code, we realized that we might be able to improve our accuracy score by increasing the number of stratified k fold splits from 3 to 10. The theory being that the more folds, the higher the accuracy score (right up until you get to the leave-one-out cross validation technique). However, the more folds you generate, the more computationally intense your model will be (aka more models needed to train). We have to be careful as the increase in folds also increases error rate. We want to verify that our accuracy increase outweighs the increased amount of false negatives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Adversity Accuracy with 10 splits 0.648696117889\n",
      "[[13927 27940]\n",
      " [16664 68436]]\n",
      "---------------------------------------\n",
      "KNN Adversity Accuracy with 3 splits 0.552151346413\n",
      "[[17548 24319]\n",
      " [32543 52557]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# X = create a matrix with all variable except response variable in it. rno_000001\n",
    "# Y = this should be predictor variable\n",
    "# create variables we are more familiar with\n",
    "DF_NoT_KNN2= DF_NoT.copy()\n",
    "del DF_NoT_KNN2['rno_000001']\n",
    "\n",
    "X2 = DF_NoT_KNN2.as_matrix()\n",
    "y2 = DF_NoT['rno_000001'].as_matrix()\n",
    "\n",
    "\n",
    "yhat2 = np.zeros(y2.shape) # we will fill this with predictions\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "KNN2_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X2,y2):\n",
    "    KNN2_clf.fit(X2[train],y2[train])\n",
    "    yhat2[test] = KNN2_clf.predict(X2[test])\n",
    "\n",
    "KNNr2_total_accuracy2 = mt.accuracy_score(y2, yhat2)\n",
    "KNNr2_conf2 = mt.confusion_matrix(y2, yhat2)\n",
    "print ('KNN Adversity Accuracy with 10 splits', KNNr2_total_accuracy2)\n",
    "print(KNNr2_conf2)\n",
    "print('---------------------------------------')\n",
    "print ('KNN Adversity Accuracy with 3 splits', KNNr_total_accuracy2)\n",
    "print(KNNr_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After re-running the test with additional splits for the statified k fold cross validation iterator, we can see that the accuracy score increased from 55% to 65%. While normally, we assume that changing to a larger k does not have much effect for a dataset with close to 130,000 records, we can see that there is quite an increase in the accuracy score. We are also optimistic that using a larger k will help mitigate the amount of bias towards overestimating the true expected error because training folds will be more closely related to the total dataset.\n",
    "\n",
    "source: http://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation\n",
    "\n",
    "\"When K is small, we are restraining the region of a given prediction and forcing our classifier to be more blind to the overall distribution. A small value for K provides the most flexible fit, which will have low bias but high variance. Graphically, our decision boundary will be more jagged. On the other hand, a higher K averages more voters in each prediction and hence is more resilient to outliers. Larger values of K will have smoother decision boundaries which means lower variance but increased bias.\"\n",
    "\n",
    "source: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n",
    "\n",
    "We can also see that our number of true positives increased from 52557 entries to 68436 entries. \n",
    "\n",
    "The number of true negatives decreased from 17548 entries to 13927 entries.\n",
    "\n",
    "Our number of false postitives increased from 24319 entries to 27940 entries. \n",
    "\n",
    "The number of false negatives decreased from 32543 entries to 16664 entries. \n",
    "\n",
    "We can see that the accuracy increase outweighs the minimal increase in the amount of error.\n",
    "\n",
    "source: http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">20 Points - Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we ran the KNN for the previous output, you will find the Random Forest Classifier code below with the SVM code following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RANDOM FOREST CLASSIFIER (Round 1: test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RANDOM FOREST CLASSIFIER for salary_range_Jr_Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Salary_Range_Jr_level 0.901084533776\n",
      "[[72339  9351]\n",
      " [ 3208 42069]]\n"
     ]
    }
   ],
   "source": [
    "###NOTE THIS TAKES A VERY LONG TIME TO RUN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "    \n",
    "RFs_total_accuracy = mt.accuracy_score(y, yhat)\n",
    "RFs_conf = mt.confusion_matrix(y, yhat)\n",
    "print ('Accuracy for Salary_Range_Jr_level', RFs_total_accuracy)\n",
    "print (RFs_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RANDOM FOREST CLASSIFIER for rno_000001 (diversity variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rno_000001 0.607890239196\n",
      "[[19343 22524]\n",
      " [27261 57839]]\n"
     ]
    }
   ],
   "source": [
    "###NOTE THIS TAKES A VERY LONG TIME TO RUN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, n_estimators=50, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X2,y2):\n",
    "    clf.fit(X2[train],y2[train])\n",
    "    yhat2[test] = clf.predict(X2[test])\n",
    "    \n",
    "RFr_total_accuracy2 = mt.accuracy_score(y2, yhat2)\n",
    "RFr_conf2 = mt.confusion_matrix(y2, yhat2)\n",
    "print ('Accuracy for rno_000001', RFr_total_accuracy2)\n",
    "print (RFr_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It seems like we were more accurate for both tasks using the random forest classifier. Let's try to rerun with different parameters and see if our model improves. we will increase the max_depth from 10 to 50, and the n_estimators from 50 to 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### RANDOM FOREST CLASSIFIER (Round 2: tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RANDOM FOREST CLASSIFIER for salary_range_Jr_Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Salary_Range_Jr_level 0.930793040711\n",
      "[[74382  7308]\n",
      " [ 1479 43798]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Sal_clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    Sal_clf.fit(X[train],y[train])\n",
    "    yhat[test] = Sal_clf.predict(X[test])\n",
    "    \n",
    "RFs_total_accuracy = mt.accuracy_score(y, yhat)\n",
    "RFs_conf = mt.confusion_matrix(y, yhat)\n",
    "print ('Accuracy for Salary_Range_Jr_level', RFs_total_accuracy)\n",
    "print (RFs_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### RANDOM FOREST CLASSIFIER for rno_000001 (diversity variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rno_000001 0.611426591161\n",
      "[[19170 22697]\n",
      " [26639 58461]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RFCr_clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X2,y2):\n",
    "    RFCr_clf.fit(X2[train],y2[train])\n",
    "    yhat2[test] = RFCr_clf.predict(X2[test])\n",
    "    \n",
    "RFr_total_accuracy2 = mt.accuracy_score(y2, yhat2)\n",
    "RFr_conf2 = mt.confusion_matrix(y2, yhat2)\n",
    "print ('Accuracy for rno_000001', RFr_total_accuracy2)\n",
    "print (RFr_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the depth and number of estimators seems to improve our accuracy level. This is to be expected since generally an increase in these two categories increases the accuracy to a point where the computational cost is too great and the accuracy increase becomes negligable. If we had more time and faster computers, we would continue changing out parameters until the accuracy increase levelled out. \n",
    "\n",
    "--------------------------------------------------------------\n",
    "\n",
    "Lets look at the type I and type II errors for the salary range:\n",
    "\n",
    "ORIGINAL OUTPUT for random forest salary range jr level\n",
    "\n",
    "Accuracy for Salary_Range_Jr_level 0.90040719242\n",
    "\n",
    "[[72210  9480]\n",
    "\n",
    " [ 3165 42112]]\n",
    "\n",
    "TUNED OUTPUT for random forest salary range jr level\n",
    "\n",
    "Accuracy for Salary_Range_Jr_level 0.931139587452\n",
    "\n",
    "[[74460  7230]\n",
    "\n",
    " [ 1513 43764]]\n",
    "\n",
    "We can see that while the accuracy increased, our false positive rate and false negatives rate decreased. This is good news for our model for salary range. Let's move forward with the tuned model and its output for salary range.\n",
    "\n",
    "--------------------------------------------------------------\n",
    "\n",
    "Lets look at the type I and type II errors for the rno_000001:\n",
    "\n",
    "ORIGINAL OUTPUT for random forest RNO_000001\n",
    "\n",
    "Accuracy for rno_000001 0.607890239196\n",
    "\n",
    "[[19343 22524]\n",
    "\n",
    "[27261 57839]]\n",
    "\n",
    "TUNED OUTPUT for random forest RNO_000001\n",
    "\n",
    "Accuracy for rno_000001 0.611426591161\n",
    "\n",
    "[[19170 22697]\n",
    "\n",
    " [26639 58461]]\n",
    " \n",
    "Additionally, our accuracy for RNO_000001 increased, while our false negative rate decreased. Conversely our false positive rate slightly increased. But, this increase seems to be minimal and does not outweigh the increase in accuracy generated by the tuned model. Let's move forward with the tuned model and its output for the diversity classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Final Accuracy Summary for Salary Range and Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Salary_Range_Jr_level 0.901084533776\n",
      "[[72339  9351]\n",
      " [ 3208 42069]]\n",
      "----------------\n",
      "Accuracy for rno_000001 0.607890239196\n",
      "[[19343 22524]\n",
      " [27261 57839]]\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy for Salary_Range_Jr_level', RFs_total_accuracy)\n",
    "print (RFs_conf)\n",
    "print ('----------------')\n",
    "print ('Accuracy for rno_000001', RFr_total_accuracy2)\n",
    "print (RFr_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plot of Weighted Coefficients for Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23467161  0.02778166  0.01576426  0.02069342  0.02772517  0.03192553\n",
      "  0.07011559  0.1203718   0.06164987  0.04885122  0.02611741  0.02340923\n",
      "  0.05137662  0.02816392  0.03221555  0.03828555  0.05898975  0.0640208\n",
      "  0.00077305  0.00299146  0.0033482   0.00177716  0.00898116]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118c8b320>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFlCAYAAAAQ3qhuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116364f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the weighted coefficients for the Random Forest Classifier that is classifying 'rno_000001'\n",
    "print(RFCr_clf.feature_importances_)\n",
    "weights = pd.Series(RFCr_clf.feature_importances_[:,],index=DF_NoT_KNN2.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see based on the output above that the determining variables for 'rno_000001' include 'Hist_yr' and 'nextwigdte'. Since 'Hist_yr' was our dataset identifier, we do not consider this variable to be informative. It is interesting to note that 'nextwigdte' or next within grade promotion is a factor of the race indicator. This information seems to suggest that the further away your promotion was (in days), the more likely you are to be classified as caucasian.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plot of Weighted Coefficients for Salary Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03373189  0.03133478  0.43070942  0.05102168  0.02904249  0.05721546\n",
      "  0.02793032  0.04302589  0.03103013  0.01227538  0.08538597  0.03703504\n",
      "  0.03262328  0.01201327  0.01417663  0.02253365  0.04121546  0.00348316\n",
      "  0.0042161 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2a1d2c0a5f8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEuCAYAAACTaJmFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx8jNzArnRG5mVjgncjOzwjmRm5kV7n8Bv2vDSPW6vOkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a1d0bd0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the weighted coefficients for the Random Forest Classifier that is classifying 'salary_range_Jr_Level'\n",
    "print(Sal_clf.feature_importances_)\n",
    "weights = pd.Series(Sal_clf.feature_importances_[:,],index=DF_NoT_KNN1.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see from the output above that the important features for the junior level salary range is grade by a large margin. This makes sense and is consistent with our data since salary range should be determined by your level of experience and skill at NASA. Grade represents the management level with 15 being the highest containing C-level employees and grades less than 5 representing students and interns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After adjusting the parameters, we did see improvements in the accuracy score for the junior level salary range and diversity classifier. Increasing the max_depth and the n_estimators for the random forest classifier actually increased our accuracy scores. You can see that we also did not normalize our weights for the random forest feature importance graphs. This is because random forest models does not need to be normalized. Since the data is being sorted into a collection of trees, a normalization would result in the same output, that is, random forest models are resistant to transformations of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logistic Regression & Support Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Logistic Regression for 'salary_range_jr_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy: 0.817245290152\n",
      "[[18816  1546]\n",
      " [ 4255  7125]]\n"
     ]
    }
   ],
   "source": [
    "###NOTE THIS TAKES A VERY LONG TIME TO RUN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "DF_SVM_sal= DF_Reg2.copy()\n",
    "\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None)\n",
    "\n",
    "if 'salary_range_Jr_Level' in DF_SVM_sal:\n",
    "    y = DF_SVM_sal['salary_range_Jr_Level'].values \n",
    "    del DF_SVM_sal['salary_range_Jr_Level']\n",
    "    X = DF_SVM_sal.values\n",
    "\n",
    "num_cv_iterations = 4\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=4, #num_cv_iterations\n",
    "                         test_size  = 0.25)  \n",
    "\n",
    "for train_indices, test_indices in cv_object: \n",
    "   \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "LRs_acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "LRs_conf = mt.confusion_matrix(y[test_indices],y_hat)\n",
    "print('LR accuracy:', LRs_acc )\n",
    "print(LRs_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SVM for 'salary_range_jr_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha_000\\Anaconda3\\envs\\py3k\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X[train_indices]) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X[train_indices]) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X[test_indices]) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y[train_indices])  # train object\n",
    "\n",
    "y_hat2 = svm_clf.predict(X_test_scaled) # get test set precitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy: 0.817245290152\n",
      "[[18816  1546]\n",
      " [ 4255  7125]]\n",
      "SVM accuracy: 0.863493163632\n",
      "[[18829  1533]\n",
      " [ 2800  8580]]\n"
     ]
    }
   ],
   "source": [
    "# COMPARISON SUMMARY OF LR MODEL VS SVM MODEL\n",
    "LRs_acc = mt.accuracy_score(y[test_indices],y_hat)\n",
    "LRs_conf = mt.confusion_matrix(y[test_indices],y_hat)\n",
    "print('LR accuracy:', LRs_acc )\n",
    "print(LRs_conf)\n",
    "\n",
    "SVMs_acc2 = mt.accuracy_score(y[test_indices],y_hat2)\n",
    "SVMs_conf2 = mt.confusion_matrix(y[test_indices],y_hat2)\n",
    "print('SVM accuracy:', SVMs_acc2 )\n",
    "print(SVMs_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Feature Importance for Logisitic Regression Model for 'salary_range_Jr_Level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.54181567e-03   9.88607060e-03  -4.31366966e-01  -1.44440889e-01\n",
      "    9.88607060e-03  -1.12806290e-03   2.76178071e-04   1.38888505e-04\n",
      "   -6.55847483e-05   6.33576105e-05  -2.68081826e-01  -2.58334461e-03\n",
      "   -9.73641974e-03  -1.18296219e-02  -2.17156925e-02  -1.08680829e-04\n",
      "    4.24534713e-05  -4.60082250e-03  -8.25913987e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEuCAYAAACESglMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2286e7865c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lr_clf.coef_)\n",
    "try:\n",
    "    weights = pd.Series(lr_clf.coef_[0],index=DF_SVM_sal.columns)\n",
    "    weights.plot(kind='bar')\n",
    "except:    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Feature Importance for Scaled Support Vector Machine Model for 'salary_range_Jr_Level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45487, 19)\n",
      "(45487,)\n",
      "[22753 22734]\n"
     ]
    }
   ],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17207667 -0.00218468 -1.17499528 -0.16452829 -0.00218468 -0.01777825\n",
      "   0.18036434 -0.02065666 -0.05668389  0.02338232 -0.58860356 -0.0666993\n",
      "   0.00218468 -0.18322886 -0.19875926 -0.38312833  0.08980091  0.03497761\n",
      "  -0.01628478]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEuCAYAAACESglMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmcXFWZ//FPSIdNGwjQgIo/UUe+Iu6OsqsggmREnR/i\nYHRAJOwOgegoCoooiBugoiBKomBEFBVlkaDIKEkAYRAVRB4MOoojYJAArZElpOePc4uuFL3UPXU7\nVX3zfb9e/epaT53anjr3LM+ZMjQ0hJmZ1c9a3a6AmZlNDAd4M7OacoA3M6spB3gzs5pygDczq6m+\nblegYenSwXGn80yfvj7Lli3v+LGqKMd1mdhyXJeJLaeX6lJVOWtqXQYG+qeMdt2kasH39U3tmXJc\nl4ktx3WZ2HJ6qS5VleO6PNGkCvBmZtY+B3gzs5pygDczq6msQVZJawFnAi8CHgZmRcSSpuvfChwN\nrABuBo6IiJWdV9fMzNqV24J/E7BuROwAHAuc2rhC0nrAScCuEbETsCHw+k4ramZm5eQG+J2BBQAR\ncR3wz03XPQzsGBGNuT19wEPZNTQzsyxTcrJJSjoH+E5EXF6c/yPwrIhY0XK7/wBmADMiYswHWrHi\nsaGqphaZma1BRp0Hn7vQ6UGgv+n8Ws3Bveij/ySwNbDPeMEdaGtRwMBAP0uXDpav7QSU47pMbDmu\ny8SW00t1qaqcNbUuAwP9o16X20WzmNQyR9L2pIHUZmcD6wJvauqqMTOz1Si3BX8R8FpJ15AODw6U\nNBN4MvDfwEHAQuAqSQCfjYiLKqiv9Zh3fvyqcW8z79jdVkNNzKxVVoAvpjwe1nLxbU2nPb/ezKzL\nHIjNzGrKAd7MrKYc4M3MasoB3sysphzgzcxqygHezKymHODNzGrKAd7MrKYc4M3MasoB3sysphzg\nzcxqygHezKymcrNJmplNenXPhuoWvJlZTTnAm5nVlAO8mVlNOcCbmdWUA7yZWU05wJuZ1ZQDvJlZ\nTXkevJlZh3p1Pr1b8GZmNeUWvFmLXm2NmZWVFeAlrQWcCbwIeBiYFRFLmq7fG/gQsAKYFxFfrqCu\nZmZWQm4XzZuAdSNiB+BY4NTGFZKmAacDewCvAg6RtHmnFTUzs3KmDA0Nlb6TpNOA6yPiguL8/0bE\n04rTLwQ+GRGvK86fDlwTEReOVeaKFY8N9fVNffz83u/+/rj1uOTUN457m14qp5fq0k457ZRRlV56\nfauyJj6nXvr8tltOL8l8faeMdvvcPvgNgAeazj8mqS8iVoxw3SCw4XgFLlu2vHQlli4dHPc2I/WV\nDgz0r3Lfdsqpqj5VlNH6nFqfz+qsy0hGqk8VuvWcJur5QP2eUy99l3LLqeK1WZ2v78BA/6i3z+2i\neRBoLnWtIriPdF0/cH/m45iZWabcFvxiYG/gW5K2B25uuu43wHMkbQz8DXgl8OmOamkTop0jATNb\nfar+TuYG+IuA10q6htT/c6CkmcCTI+JLkuYAV5COEOZFxP9m19DWCP6xMateVoCPiJXAYS0X39Z0\n/SXAJR3Uy8zMOuSVrGZmNeUAb2ZWUw7wZmY15Vw0VhvtrHkwW5O4BW9mVlNuwZv1MB+VWCfcgjcz\nqykHeDOzmnKANzOrKQd4M7OacoA3M6spB3gzs5pygDczqykHeDOzmnKANzOrKQd4M7OacoA3M6sp\nB3gzs5pygDczqykHeDOzmnK64NXIqV/NbHXq2QDvYGhm1pmsAC9pPWA+sBkwCBwQEUtbbnMMsF9x\n9gcRcWInFTWz7mtteLnR1dty++APB26OiF2A84Djm6+U9CzgbcCOwPbAHpJe2ElFzcysnNwAvzOw\noDh9ObB7y/V3Aq+LiMciYgiYBjyU+VhmZpZh3C4aSQcBx7RcfA/wQHF6ENiw+cqIeBS4V9IU4FPA\nTRFx+1iPM336+vT1TR23wgMD/ePeph1VlVNFmVXUpddel7o9p4n4vHRSbi89p7LlXHLqGyt53Has\nia9vs3EDfETMBeY2Xybpu0DjUfuB+1vvJ2ldYB7pB+CI8R5n2bLl41a2qv6+ieo3zCmzirr02utS\nt+c0kf3M/sxUW06rNeH1HesHIHcWzWJgBnA9sBewsPnKouX+feCqiPhE5mOYmVkHcgP8WcC5khYB\njwAzASTNAZYAU4FXAetI2qu4z/sj4toO62tmZm3KCvARsRzYd4TLT2s6u25upczMrHNOVWBmVlMO\n8GZmNeUAb2ZWUw7wZmY15QBvZlZTDvBmZjXlAG9mVlMO8GZmNeUAb2ZWUw7wZmY15QBvZlZTDvBm\nZjXlAG9mVlMO8GZmNeUAb2ZWUw7wZmY15QBvZlZTDvBmZjXlAG9mVlMO8GZmNeUAb2ZWUw7wZmY1\n5QBvZlZTfTl3krQeMB/YDBgEDoiIpSPcbi3gMuD7EfHFTipqZtZs3rG7rXJ+YKCfpUsHu1Sb3pTb\ngj8cuDkidgHOA44f5XYnAdMzH8PMzDqQG+B3BhYUpy8Hdm+9gaQ3AyubbmdmZqvRuF00kg4Cjmm5\n+B7ggeL0ILBhy32eD8wE3gx8qJ2KTJ++Pn19U8e93cBAfzvFrbZyqiizirr02utSt+c0EZ+XTsrt\npefUS+W4LqsaN8BHxFxgbvNlkr4LNB61H7i/5W77A08DrgK2Ah6R9D8RMWprftmy5eNWtqo+tonq\nq8sps4q69NrrUrfnNJF9u/7MVFfOmlqXsX4AsgZZgcXADOB6YC9gYfOVEfHexmlJHwbuHiu4m5lZ\n9XID/FnAuZIWAY+QumOQNAdYEhEXV1Q/MzPLlBXgI2I5sO8Il582wmUfznkMMzPrjBc6mZnVlAO8\nmVlNOcCbmdWUA7yZWU05wJuZ1ZQDvJlZTTnAm5nVlAO8mVlNOcCbmdWUA7yZWU05wJuZ1ZQDvJlZ\nTTnAm5nVlAO8mVlNOcCbmdWUA7yZWU05wJuZ1ZQDvJlZTTnAm5nVlAO8mVlNOcCbmdWUA7yZWU31\n5dxJ0nrAfGAzYBA4ICKWttxmL+AEYApwI3BkRAx1Vl0zM2tXVoAHDgdujogPS9oPOB6Y3bhSUj/w\nKeDVEXGvpPcCmwJLRyzNrIbmHbvbKucHBvpZunSwS7WxNVFuF83OwILi9OXA7i3X7wjcDJwqaSFw\nT2sL38zMJta4LXhJBwHHtFx8D/BAcXoQ2LDl+k2BXYEXA38DFkq6NiJuH+1xpk9fn76+qeNWeGCg\nf9zbtKOqcqoos4q69NrrUrfn1Et1qaqcXqpLVeW4LqsaN8BHxFxgbvNlkr4LNB61H7i/5W5/BW6I\niLuL219NCvajBvhly5aPW9mqDnEn6lA5p8wq6tJrr0vdnlMv1aWqcnqpLlWVs6bWZawfgNwumsXA\njOL0XsDClut/Djxf0qaS+oDtgVszH8vMzDLkDrKeBZwraRHwCDATQNIcYElEXCzp/cAVxe2/FRG3\ndFxbMzNrW1aAj4jlwL4jXH5a0+kLgAvyq2ZmZp3wQiczs5pygDczqykHeDOzmnKANzOrKQd4M7Oa\ncoA3M6spB3gzs5pygDczqykHeDOzmnKANzOrKQd4M7OacoA3M6spB3gzs5pygDczqykHeDOzmnKA\nNzOrKQd4M7OacoA3M6spB3gzs5pygDczqykHeDOzmnKANzOrKQd4M7Oa6su5k6T1gPnAZsAgcEBE\nLG25zbuBmcBK4GMRcVGHdTUzsxJyW/CHAzdHxC7AecDxzVdK2giYDewA7AF8ppNKmplZebkBfmdg\nQXH6cmD3luv/DvwBeFLxtzLzcczMLNO4XTSSDgKOabn4HuCB4vQgsOEId70TuBWYCpwy3uNMn74+\nfX1Tx7sZAwP9496mHVWVU0WZVdSl116Xuj2nXqpLVeX0Ul2qKsd1WdW4AT4i5gJzmy+T9F2g8aj9\nwP0td9sLeArwzOL8FZIWR8T1oz3OsmXLx63swEA/S5cOjnu71VVOq5wyq6hLr70udXtOvVSXqsrp\npbpUVc6aWpexfgByu2gWAzOK03sBC1uuXwb8A3g4Ih4i/QBslPlYZmaWIWsWDXAWcK6kRcAjpNky\nSJoDLImIiyXtDlwnaSWwCPhRFRU2M7P2ZAX4iFgO7DvC5ac1nT4BOCG/amZm1gkvdDIzqykHeDOz\nmnKANzOrKQd4M7OacoA3M6spB3gzs5pygDczqykHeDOzmnKANzOrKQd4M7OacoA3M6spB3gzs5py\ngDczqykHeDOzmnKANzOrKQd4M7OacoA3M6spB3gzs5pygDczqykHeDOzmnKANzOrKQd4M7Oa6uvk\nzpL+Fdg3ImaOcN3BwKHACuCkiLi0k8cyM7Nyslvwkj4LnDJSGZK2AI4CdgL2BE6RtE7uY5mZWXmd\ndNFcAxw+ynWvABZHxMMR8QCwBHhhB49lZmYljdtFI+kg4JiWiw+MiG9KevUod9sAeKDp/CCw4ViP\nM336+vT1TR2vOgwM9I97m3ZUVU4VZVZRl157Xer2nHqpLlWV00t1qaoc12VV4wb4iJgLzC1Z7oNA\nc636gfvHusOyZcvHLXRgoJ+lSwdLVmXiymmVU2YVdem116Vuz6mX6lJVOb1Ul6rKWVPrMtYPQEeD\nrGO4HjhZ0rrAOsA2wC0T9FhmZjaCSgO8pDnAkoi4WNLngIWkfv7jIuKhKh/LzMzG1lGAj4ifAD9p\nOn9a0+kvA1/upHwzM8vnhU5mZjXlAG9mVlMO8GZmNeUAb2ZWUw7wZmY15QBvZlZTDvBmZjXlAG9m\nVlMO8GZmNeUAb2ZWUw7wZmY15QBvZlZTDvBmZjXlAG9mVlMO8GZmNeUAb2ZWUw7wZmY15QBvZlZT\nDvBmZjXlAG9mVlMO8GZmNeUAb2ZWU1OGhoa6XQczM5sAbsGbmdWUA7yZWU05wJuZ1ZQDvJlZTTnA\nm5nVlAO8mVlNOcCbmdWUA7yZWU31fICXtGtF5bytonKeI2mGpC0lTcks40UV1GPDTsuos07fp9z3\nttdV9Pmt5LNXRV2qKkfSrJbzR+XWpyqSNu60jL4qKjLBTgT+q4JyDgG+3kkBkt4F/CuwMXAu8E/A\nuzKKOknSJsBXgPMj4u8ZZVwG7JxxPwAk7THadRHxw4zyng+cBUwH5gO3RMSlJcvoB94HPBW4FPhV\nRCzJqEsV79MVwKivUYm6rAccCgj4NXB2RDyaUc5rgGcD1wG3R8RDGWVU9fnt6LNXZV06LUfSW4E3\nALtK2q24eCrwfOBzZetTBUmvAr4ATJV0IfCHiJibU9ZkCPBDki4CAlgJEBEfyChnHUk3tZQzs2QZ\n+wGvBH4cEZ+RdENGPYiIvSVtAfw78ENJv4mIWePdr8V9kmaz6vMpE5jfOsrlQ0DpAA98FjgQ+DIw\nF7icFKTLmFfc71XA3UU5r8qoSxXv0zJJb2TV1/f2jHK+UZSxANiJ9KP+9jIFSPoYsCWwDfAw8H5G\nf//GUsnnl84/e1XWpdNyFgB3AZsAZxeXrQTuKFOIpK1Huy7jc/NR0nP6DvAxYDHpu1DaZAjw8yoq\n530VlLEWKQA2Evg83EFZ04B1SK2FFRn3/yvw4uIPygfmQzMec0wRsUTSUEQslTSYUcQmETFP0tsj\n4hpJuV2IVbxPmwFHN50fAnYb5bZj2SQiGp+970tamFHGzhHxSkn/FRHnSjo8owyo7vPb6Wevyrp0\nVE5ELAN+AvxE0gxgW9IRUtn3aR7wLOA2oLmbKOdzszIi7iu+Sw9lfpeAyRHg9wXOAS6JiMc6KOdU\nUtfBeRFxX2YZ5wNXA8+Q9APgezmFSLqKFNznAq/J7KK5FfhqRCzNqQOp9dWaaW5KcdmzMsq7T9Kh\nwJMk7Qfcn1MpSc8t/m9J3g8fpFZzp+/TmcBFEZFbh4ZfS9opIhZLegHwB0nTgCkR8UibZfRJWpd0\nNDsVyP0eVPG6QOefPajou1RVOZJOAbYGFgIHSHplRLy7RBF7AD8F/j0i/jenDk2WFPXZRNKxwB9y\nC5oMAf49wDuBEyT9EDgnIn6bUc7uwEzgEkl3FuVcWaaAiPi8pB+T+uciIn6VUQ+A2RFxs6SNM4M7\nwCBwkaRGV8aCiGg7NWhEPDPzcUdzEPAB4F7gn0nvWVlHkbowtgG+DeS2VM8CrqR4n4A/ZpTxMuA4\nSVcCcyPiN5l12QXYU9KjpKM2gNsp90N6OnAjMAD8rDhfWkScUTyf5wO3RcTNOeXQ4WevqMvni4bO\ntp3UpapygFdGxE4Akj5LGusoU4/lkg4D/h/QaYA/DJgFLAL+BhycW9CkSRcsaVPSoMc+pF/sD0XE\ntRnlbAN8kBTwfw98PCIuGuc+Hxrtuoj4SEYdHh9EAToaRJG0LXAcadBrHvDZ4rCz3fu/ATiSFHym\nkLoUXphRj+Mj4qSm86dExPtLlvH65oFZSW+JiG+VuP8WwAbAeaTxjSmk1/jciHhFmboU5a0F7EX6\nsdqCNL7w9ZxB0k5Jmk4aQPx9RNybWUZrd+ejwJ3AF8p8ZprKy/7sSToY2Doi/rNouH0tIr6WUYeq\nyrke2D4iVhbv+zURsX3ZcjpR9cQHmAQteEl7Ae8gteq+RuoXnQb8AGh7uqGkI4D9gQdJXT4HFOVc\nB4wZ4IF7iv9vIv0oLAZeTvq1ztHxIIqkjUgDTPuTukNmk4LZpaTBvHadROqPP4w0W+m1JetxEKm1\nsU3RhwmpX3Rt0mBgO2W8nlTnt0rasamMNwJtB3hge9LrIOBLxWUrSTNiSimm2+1Ben2fQZqBtSlw\nCfC6EuUcSnp9121cFhHPa/O+X+GJ3WhIIiJyjpDWIw0eLiS9Vi8H/kKaffKGdgup6LN3OND40f0X\nUqOtdGCusJxvAoslXQdsB1xQ5s5FF9phwGuADUmvy0Lg8xHxjzaLqXriQ+8HeNKMg7Mi4ifNF0r6\nRMlynga8NSJ+33TZo8UXcEwRcXbxmPtExBHFxV+X9KOSdWioYhDlBtKYwn4R8XgXhKSXlCznroi4\nVtJhEfFVSe8oef/5pO6Q44CTi8tWkgJHu35JCp7/IHWpNMoo9SWLiO8B35M0IyJ+UOa+I/gt6Qv6\nuYhY3LiwaLWWMRuYAZRuITP8/A8HrmG4YVH6aKQwEBGNIHKFpB9GxAclXV2ynCo+e481xjci4lFJ\nuV0JlZQTEadKuoLUODgnIn5dsoivAL8gfQ8GgX7S0d/5pGmc7dThwJEul/SUknV5XM8H+IgYbYHS\nLNKLNyZJ+zeKAnaRtEtT2eeV7ObZWNKzI+IOSSL9UufIHkSR1Fd8oF9IMdgmaW2AiHgkIo4rWZeH\nJb0SmCZpT1KgLWO74v98Uku34Zmk1tS4IuJO4KuSzi3bl9usucUr6c0tj1G2xftS4NkRcVMxXfIH\nEfHoaF/CMfwKuDNngkBEXAEg6d0R8cni4sUdNCw2kPTciLit6KrsV1qP8eSS5Ww90vtU8rPXmFF0\nPem1vrhkHSotp5jmeDIpwN9SvOZlBjef2vTj2fCrnFlTkj5C+lFfG1ifNGZTtmEBTIIAP4Z2V6xt\nU/zfjtRCvIbUCppG6qst42jS4NLmwJ9Ih2Q5OhlEOY80WHwrqx6+D5EWwpR1OPBcUlfNR4u/sven\neOy1Sa27l5Ce16vbKUDSXaT6ryNpfVK/8JbAXyJiqxJ1qbLFO4+0oOcm0pf+30ive1lXAb+TdAfF\nLKWIKDtt7slKi3BuAHakqbunpCOB+ZKeSnqN30V6XiePea9Che8TEXGSpEtJr+15EfHLMvevuhzS\n9+pE0udmZ+CrQJlV9A8VjckFwAOkFvwM0vegrDeQXtfTgdNIM7qyTOYA31ZLrzHQJ2lBRPxL4/Ji\nQKaUiFhEajk3ypg2xs2foGUQ5XfFH6RA2FZ9Ynhx1sdIPzjrF+dzl3of2DQ4uk9xZPHNdu/caLVI\nugx4Y0SsKKbyXVaijKcUZcwH3h8RdxZBqNRskYpbvE+LiK8U5X5SUu5q6kOBt5A5bbTwTuBTpGl8\nvyaNH+V4GWkQ+mFgc9Iq6ue0e+eq3qeijKeTxjjWTWf1xjITFiTNiohzis9rIxa8SNK/Rd5CyL9H\nxOXF6cskzSl5/5nAh0hdcv2ksb7F5L1Xd0XEw5L6I60tWTujDGByB/iyNpO0UUTcXxyWblK2gKK/\nfg7DM05WAG1/Qah2EOUwUgvh7pL3A6oZHG3R3E/YR1ooVNaziu4aIuLPknIHsato8Q5J2joibpf0\nbNIgYo4/ATdExMrM+xMRtwF7N8530Cd7BGll8PGk2VtHj33zUVXxPl1IGru5M7MOjfvdlnn/J5Qn\n6XjSEdfLSF2Xe0B7M1gi4q/AbKXZfhsCyyJ/vc2fJL0T+HvxA7ZRZjmTOsCXbbGeBPxSUmPhSk4O\njiNJre3cL8jBRQs3+xe5yb0l+whbzQd+TJq7njs42mwuaVHPLaT+wo9nlPEbSV8j9afuSJr7naOK\nFu8xwDeL7rg/k98dtw7pc3cLRUszSqbIkPTR4vE77ZP9c0TcVbQMfyLphIwyAG6t4H0ajIjjMx//\n8aM10gDml4DLOxm/YbiLs9HNeQ+pQdZW40vSyxme+jxIGu+YAhwZEdeUrMuhwNNJMeYd5HUNApMg\nwEvaMiL+1HReERGkPugyHiINSq4gtVRLJ2ui8y9Io/+8dRVp2/3nSnlJANYuRv1/znDgaPvQNCIe\nBv5H0jGkBGGPkhKynUfGyrmI+IJSYqRnAb8tWjRlHUz6wm4NfCMicgfelpJmv/xIKRlV6ZZURPxM\n0quBrYA7IiKnLxXglMz7NdubavpkH5D0JtLRyaGUH1BvOIT0Pj2H/PfpFqUVzzcx/PnNyfXzUVIO\npI9J+h5pUVrOUcHiiDincUbSURFRJtnY6cA+zY9dHNlcyPBEhDE1TQhp9gBp4WDZeAf0cIBXyk74\nNOATkt5bXDyV9IV5cUQcWbLIE4DtIuVJ2YK0pLnsQoaOviAV9Z9Hy/9OfRv4ImkB2a2k1tCeZQtR\nWoY/j9TyuEvSOyPippLFNLfoXlxMSbwT+GaUW1x0ASn5GaTgPh94fZmKSNqnqE8f8C2lKa0njXO3\nkfyclgyZGWVU1Sc7i7RY6v3Au4H/yCznyaTvzrbA5pIWZXRHNOeygcxcPxFxI3Cj0kKws4AlpKOm\ntmjkbJJrAS+gXDbJaSP8sNxJm2OFhXNIjatLSA3QjlNW92yAJ7Uq9yMNBjX6rleS33oZjCJ3RkTc\nLSknRcDBpJZ2p1+Q7P7ziDg38zFHsz5patnsiNhf0u6Z5XwOmBURv5T0YtLhapkFV5AWrv2D4YU4\nTydl+tuTtDK1XU+KYkVsRJyvllzfbZpT1GEBqXvvv4v/ZVWRIbOSPtmIGCS1mCF9fnPNI+Vd+Trp\nuXyVEgulirrsqpRXfis6OEJSmvb8DtJsqQtJqU3KqCSbJGlg9kpSd05jFs2epAWZ7XoaKea9npRe\n4+vRsv6nrJ4N8JGyuS2U9NKI+DmkpeNlB6uaujT6iulUi0jT5nKy1307IhozYTr5gnTaf16ltUkj\n/zdKeh7wpMxypjSmqEXEL5rGOsrYKCL2KU6frbQQ598lLSpZziOSXktapfwKipS2JT1WtJqHImIo\ns0EA1WTIfC9p9kvHfbIV2SQizihO/0Itaw7aUeER0tGkFBKzcvrgY9Vsks0590sdkUTER5QWeu1M\n+rF4EHhfI3a1WcZS4AzgjGJg/22SPgDcGCXTfjT0bIBvso3SIoR1gE9K+lREfLrE/Ufq0vh+Zl06\nyhFeVf95xd5DSglwMmnV8OzMch5TSjmwkJSGIecHdCNJm0bEvcVMpw2Lqajrj3fHFrOAT5OOKm4l\nLzXyIknnA1tK+iJpRk4WdZ4h85KIaGywccaYt1w91pO0RXEkvDl5M4yqOkLaICIWZNxvFaom5/5K\nUpxaj9TFkjvzCtJ44aOkH/Z/yi1kMgT42aQlvxeQcr/8kPTlbUvFXRqb8cQAWKbfsOr+845FSmP7\nW9IHKXdQE9LMlU+TZs/cSl4GvBOAn0l6kNTP+x+kI6VSeXqKfup9SH2YO5A3Fe9MUu6h35AG8fYZ\n++ajas2QecTYNx9RFRtsVOl40vqC5aQf35z3uqojpPs6aXQ16SjnvlJCwu1IeY9+R+qi+bCkn0fE\nB9ssYwvSmom3AH8npXfeIyIeLFOXZpMhwDdmuwwWH4hu1nlr0pjAUtIA60NFcDwiIsZdTDMB/ecd\nk3Qm6Qf0Lobzwe845p1GEBF/KObWr0e5gaXmMi5Vyuk9QFodOURq4ZUi6TOkwPwM0vL1eyg/VfLr\nwIdJU2M/QJq9Unp/4Ii4hfQj01rHEyLixDaLqWKDjSo9SvperksKQjmppxdJ+gadHyFVtTFLpzn3\nXxsRuzRfIOkMUndPWwGetGYiSAsN7yF1n+6nlFzuS2PecxQ9v+k2abDjOmBeMS0xNwd7Fa4Gti1W\n9D0X+C4pOJZd3t9LXkHKubJjROwQEaWDO4Ck80gDeJeTWjFtB2ZJny/+X0saI7mI1EIsO3+44eWR\nEsTtEBGvIx16l7WS9H5vFBEXkNePP5YyA60ntPwdp5KrqCv2UVKe+ztJ3Sqlj0qKLslzSf3nl0a5\nzTWay9mVdKR1NLB3lE8D0fAZ0nz+55Ny7pedzDFN0lYtl21Fuc/NSaTsqVNI6amf0vSXpedb8BFx\noKQnR8TfJN0QEfeMf68Js2UxB59ICceeUXQHdLrrTzfdQWqJLe+wHEVETi4cGP6BPIDOtkFsmCrp\nZaR5/muTDpfLmgZ8Erha0q6k1lSVykyBu5T0I3Ub6ShyOanF+d6ImF9xvdrRyIZKlMyGqiLnv6RD\nioseAJ4q6ZCcVmqFg7XvIs36eg55OfcbearWJg2wNlJCtL1ALiI+3DhdHEU0uhh/VrIuj+vZAK9i\nA4niMG5IUuPynM2yq3KXpI+TEhLtCNxdzNZod+u1XvR00jZySxge8M1pxV8vPb4IrZSmH+3GBtXf\nIWVvbDePdqvzSC2wd5KC9Nlj33xEB5Jy488lDULn5n8ZTZlurN8DuxWDz9NJ86UPJh0tdSPAd7Kl\nXCNFSHartEVVg7VDpLGSAFYWcabMwsHrgJdI6ic1KAaLaamljdDFeDdp9lRpPRvggUENZ2cbYrjF\n080tqPawdbbkAAAJs0lEQVQnreLbC7iF1Ef7EvJ2uO8qFcmaSF/O5i9o7uv7AHCDpL8xnDXxqWUK\niIiXKaWxfQNwpaS/RERbubRbyjmT4UPsrHwrkbaFbGwNWWbTkYmweaNFGRHLJG1etKCr7jZqV3M2\n1L9TYpC1aRxKFTXUqhqsbd3tqpRi1tcHadrwQylV8IkRUTb9x8sj4uhiwHdXpW1Cs/RygN+i+IMU\nQM9neBCwKyLiIZ64uq30toE9ojGzpOMpZoXdgI2jg02qiwVSuzM8SFZqH1RJ346IN2s4re3jjYKy\nPzarQZkumhuLI9lrSYfsv5D0bwzvNLZaFe/xFzssZm1JLyTl1WnMfsk5Eq5kOmsFEyDOJe0k9SGG\nN/yYQYpbZRcPVtHFCPRwgG+e2C9p+y7OE6+lKJI1VTiz53bSDKNONhz+KWmK2XGRsSNTRLy5+F/V\n4X/HJK1HmocvUuKzsyOlXRgp78iIIuJIpb1ztwHmR8RlSn2W3Zxw0Cmx6nqUMpuQN6tqOmunNoiI\n5jTbDwIXSCqbUgXSj8WZpOfzCTr4Me3ZAN9icuwMvmbbidTi+Cvp/cppNW9CWgm4p6R3k6ZKtt39\npVH2MIWsHZ2q0hhXWEB6jb4CvD1KJsSKlNDr4qbzIekq8qYEdl1EvAAe79q4L2cVaqGS6awV+Esx\nF751w4+72i2gmEXWOPJcSRprmUKaX5/VhTRZArz1uCixccQYNiLl43gGKWVC2XQOVe9hWoVNIuJ9\nxenG9nJV6TgZVbcobRN5Jmm154WS/hARpRa0FRrTWY+LiAsk5Sy6qsLbSZ+79zG84cc1lBuc36/q\nSvVsgG/MniF9iLct+tmA8vm0beJpOJvklqRR/5xskgtIWT5PjvKbHk/EHqZV+LWknYoVwy8gzVia\nRsrd0+nsq8l8ZHsSKaXFd0jZVRdTcsVyYaKns7almC76eVKqjg1Jm6zfUuY9jgnIT9WzAZ5V+506\nHdCxiVdFNsnrmucwSzovItruq25S1R6mVdiF1OX0KCkYQRqvyO1zrovGXPqhsnPpW0z0dNa2SPoX\nUirz35L2Ye0HnivpAxHxvW7UCXo4wEfET7tdByslO5tkMRB1PDBd0v8vLl6LzE0OeOKOTjk/EpWI\niG2hkr7mkUzaLhqG59JvmjGX/nE9NJ31OFI+m8fzxiilQ76SdFTaFT0b4G3Syc4mGRFfAL5QtHY+\nNu4dxrdzRDTvYXoU5TZvqEyFfc0juaqicrqhMZd+IanF262+86pM44mrwf9Bl7vRHOCtKlVkk9xA\n0pRiwcqGwDkRsW+7d1Z1u/NUqeO+5mK19ByadiqKiN0iYjLnQJpGSkzXSKk7mccTIO2E9nOlvQse\nIKUq2Jnufe4AB3irzlFlgvEoHgJ+LOlzwImkKW9lVLU7T5Wq6Gs+nbQiNyftca/6Bim3zuU0TR/t\nao06EBFflnQxacbWBqRZNB9ppOGQtF1EZOeUyeUAb1V5nqSNIuL+Dso4kbTI41ukLQRLLcKKinbn\nqVgneVsa/hgRV1Zcr27bJCKOLU5XPX20K4pgfskoV59CF9YsOMBbVZ4H/FXSvaRWc85Cp5+SUrZu\nBXxR0ksi4pCx7/JEqmZ3nqq05m3J2R/2L8Uy/JsYTgiXlR+8h0zk9NFe1JUBcQd4q0REPKOCYj4R\nEZcVp99QDI7m6Gh3nop9JiLe1TijlDe/7Kye3xf/G7mZJnt/NQxPH32E4bnrdZ4+2pX3zAHeKiFp\nR9JskUY+mlkR8YuSxVwt6aOk1ayXUm5H+mad7s7Tsaapnxs3Tf2cQompn5K2jIg/kfqr62a7iPhb\n44ykp5dN32Djmww7OtnkcAYws0j09Q7K74gDaSXs70mbLtxN3spG6Hx3no5FxBeK1+Jk0kKcXUkD\nimV2LppT/D+btNjvi02nJ7tFRddMY9OObm5BuDq4i8Ymtfsj4lZI+5Aqbchc1iYRMU/S2yPiGkm5\nDZBOd+ep0mtIuVLeRdp0+3TaTIYVEXOK/yPeXuX2de01M4G5ku4BVpCmktaCpI1JA/zNn73zx7jL\nhHEL3qryF0nnSJop6VRgLUmHaHhrtrZIem7xf0vSFz9HY3eefYA5xaBrt6wkLeaZiL1dy+zr2msa\nLdp1SK/JZN728nGS3kKauXUccJ2kt0OaRtmN+rgFb1W5rfj/HNIc4J+StmUrM7h0FKmbZhvgQjI2\ncy50tDtPxSYyGdZkTlXwLVLemN+RjnIWAdt2tUbVOAZ4aaQ9pPtJq427sa0i4ABv1XmsJVHYKc2b\ntrTppcB04H7SjJHvkDGjosJNTKowkcmwJvNsmmOBr1KkcGA41fNkt7IxeBwRg5Ie6mZlHOCtI5IO\nIs3t3kbSjOLiqaSWa9kA/z5gb2q0YrOHkmH1mv/kiSkcJnPqhYbfFV2UV5OeXzdXUTvAW8fmAz8m\n7aZzcnHZSqDsRsMAv4uIJVVVbA0wmbtoqkoX3GsOJG3R+FrSlNhjx775xHKAt45ExMOkrfqOIXWv\nPAocApxH+WX5yyVdDvyC4RWba/xevMVc/neQdrq6irSRxL10MQ1yBapI4dCLHiPtQ3BzcX4HUmu+\nKxzgrSrfJs3P3ofUcvkSsGfJMnIXNtXd2cCfSa3CG0g/njMm+cKg1hQOkz1dcMN3gU1J3YxTSA0V\nB3ib9NYnbQo9OyL2l7R72QJ6bHC0lzw7ImZJ2jkiLilavJNaRKygHgu2Wm0eETt2uxINngdvVVkb\nmA3cKOl5pE2zrRp9kjYFKKbeVTmX3qoVksom2ZswbsFbVd5DmgZ4Mimv9+zuVqdWjifNMnkKaRHN\n0d2tjo1hJ+CPkpYW53OyqlZmytDQZJ5Ka71E0lNI0yOnAE+NiGu7XKVakTQQEUvHv6V1i6SFEbFL\nt+vR4Ba8VULSXNKMgSeR+uPvALbvaqVqQtKhpKl360oCICKe19VK2WhWSroICIqutG7OBHOAt6q8\niLTU/GzSnPhvd7c6tTIbmAEs63ZFbFy9lCbDAd4qc1+xWfaTIuLeRkvTKvEr4M6IWO157a2cXpsJ\n5gBvVflvSe8B/izpG8B63a5QjVxFWgJ/B8Xc6ohY7ft72uTjAG8dKVYjDpECTyN75NakjTasGocC\nbyElYTNrmwO8deq2ES67eYTLLN+fgBsiwvPfrRRPkzTrcZIWkPapvYXhHD0zu1opmxTcgjfrfad0\nuwI2OTlVgVmPkvT6xskR/szG5QBv1rs2Lv6fSdrhqvH3zK7VyCYVd9GY9a5pkq4lpdPdq7hsLfJ2\ny7I1kAO8We+qcrcsWwN5Fo2ZWU25D97MrKYc4M3MasoB3sysphzgzcxq6v8AgZNea31DT70AAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2286e6df550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(svm_clf.coef_)\n",
    "try:\n",
    "    weights = pd.Series(svm_clf.coef_[0],index=DF_SVM_sal.columns)\n",
    "    weights.plot(kind='bar')\n",
    "except:    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Interpretation of SVM scaled feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Comparing between the Logistic Regression and the SVM, the SVM provided a better accuracy score for the Junior Level Salary Range.\n",
    "\n",
    "LR accuracy: 0.817245290152\n",
    "\n",
    "[[18816  1546]\n",
    "\n",
    "[ 4255  7125]]\n",
    "\n",
    "SVM accuracy: 0.863493163632\n",
    "\n",
    "[[18829  1533]\n",
    "\n",
    "[ 2800  8580]]\n",
    "\n",
    "We can see that The SVM generated a higher accuracy level with a lower false positive score and a lower false negative score. \n",
    "\n",
    "Based on the graphs above, we can see for both LR and SVM that grade and step_emp are the greatest incfluences of the Junior Level Salary Range. This is indicative that NASA HR practices are correct in maintaining a payscale that is based on skill and experience, not race or gender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SVM for Diversity Classifier (RNO_000001 = Caucasian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 126967 entries, 2 to 17523\n",
      "Data columns (total 20 columns):\n",
      "Hist_yr                  126967 non-null float64\n",
      "bdyr                     126967 non-null float64\n",
      "grade                    126967 non-null object\n",
      "hilev                    126967 non-null object\n",
      "hiyr                     126967 non-null int64\n",
      "install                  126967 non-null object\n",
      "lastpromodte             126967 non-null float64\n",
      "nextwigdte               126967 non-null float64\n",
      "retdiscdte               126967 non-null float64\n",
      "secyr                    126967 non-null object\n",
      "step_emp                 126967 non-null object\n",
      "time_in_grade            126967 non-null float64\n",
      "age                      126967 non-null float64\n",
      "eodyr                    126967 non-null int64\n",
      "experience               126967 non-null int64\n",
      "service                  126967 non-null float64\n",
      "retpot                   126967 non-null float64\n",
      "salary_range_Jr_Level    126967 non-null uint8\n",
      "rno_000001               126967 non-null uint8\n",
      "IsMale                   126967 non-null int64\n",
      "dtypes: float64(9), int64(4), object(5), uint8(2)\n",
      "memory usage: 18.6+ MB\n"
     ]
    }
   ],
   "source": [
    "DF_Reg2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy: 0.666152101317\n",
      "[[  178 10331]\n",
      " [  266 20967]]\n"
     ]
    }
   ],
   "source": [
    "###NOTE THIS TAKES A VERY LONG TIME TO RUN \n",
    "###runtime approx 30 mins\n",
    "\n",
    "DF_SVM_div= DF_Reg2.copy()\n",
    "\n",
    "lr_clf2 = LogisticRegression(penalty='l2', C=1.0, class_weight=None)\n",
    "\n",
    "if 'rno_000001' in DF_SVM_div:\n",
    "    y2 = DF_SVM_div['rno_000001'].values \n",
    "    del DF_SVM_div['rno_000001']\n",
    "    X2 = DF_SVM_div.values\n",
    "\n",
    "num_cv_iterations = 4\n",
    "num_instances = len(y2)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=4, #num_cv_iterations\n",
    "                         test_size  = 0.25)  \n",
    "\n",
    "for train_indices, test_indices in cv_object: \n",
    "   \n",
    "    X_train2 = X2[train_indices]\n",
    "    y_train2 = y2[train_indices]\n",
    "    \n",
    "    X_test2 = X2[test_indices]\n",
    "    y_test2 = y2[test_indices]\n",
    "    \n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object):\n",
    "    lr_clf2.fit(X2[train_indices],y2[train_indices])  # train object\n",
    "    y_hat3 = lr_clf2.predict(X2[test_indices]) # get test set precitions\n",
    "    \n",
    "LRr_acc = mt.accuracy_score(y2[test_indices],y_hat3)\n",
    "LRr_conf = mt.confusion_matrix(y2[test_indices],y_hat3)\n",
    "print('LR accuracy:', LRr_acc )\n",
    "print(LRr_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.4/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X2[train_indices]) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X2_train_scaled = scl_obj.transform(X2[train_indices]) # apply to training\n",
    "X2_test_scaled = scl_obj.transform(X2[test_indices]) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf2 = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf2.fit(X2_train_scaled, y2[train_indices])  # train object\n",
    "\n",
    "y_hat4 = svm_clf2.predict(X2_test_scaled) # get test set precitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy: 0.666152101317\n",
      "[[  178 10331]\n",
      " [  266 20967]]\n",
      "SVM accuracy: 0.668924453406\n",
      "[[    0 10509]\n",
      " [    0 21233]]\n"
     ]
    }
   ],
   "source": [
    "LRr_acc = mt.accuracy_score(y2[test_indices],y_hat3)\n",
    "LRr_conf = mt.confusion_matrix(y2[test_indices],y_hat3)\n",
    "print('LR accuracy:', LRr_acc )\n",
    "print(LRr_conf)\n",
    "\n",
    "SVMr_acc2 = mt.accuracy_score(y2[test_indices],y_hat4)\n",
    "SVMr_conf2 = mt.confusion_matrix(y2[test_indices],y_hat4)\n",
    "print('SVM accuracy:', SVMr_acc2 )\n",
    "print(SVMr_conf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.84352085e-03  -3.01494415e-03   8.44899853e-02  -1.36761349e-02\n",
      "   -3.01494415e-03   1.90487674e-03   3.37056341e-05  -1.09913181e-04\n",
      "   -3.80013217e-05   4.35559156e-05   3.45148045e-02  -1.54453855e-03\n",
      "    5.89566491e-03  -2.25941694e-04   2.78900245e-03   1.73424082e-05\n",
      "   -9.43000814e-06  -9.46668609e-03   7.16095857e-02]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFUCAYAAADf+HxmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d18898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lr_clf2.coef_)\n",
    "try:\n",
    "    weights = pd.Series(lr_clf2.coef_[0],index=DF_SVM_div.columns)\n",
    "    weights.plot(kind='bar')\n",
    "except:    \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45487, 19)\n",
      "(45487,)\n",
      "[22753 22734]\n"
     ]
    }
   ],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -5.93976741e-05  -4.10404488e-04   4.13078293e-05   1.74665627e-05\n",
      "   -4.10404488e-04   9.95508032e-07  -5.47959625e-06  -5.72055385e-05\n",
      "   -8.53824433e-05  -1.60137810e-06   2.74152495e-05  -3.25121063e-05\n",
      "    4.10404483e-04  -1.39036186e-03  -1.07377318e-03  -2.57317717e-03\n",
      "    1.23716354e-05  -5.21345688e-05   1.65852464e-06]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2286cf43978>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAFUCAYAAAA+v6tyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2286c249ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(svm_clf2.coef_)\n",
    "weights = pd.Series(svm_clf2.coef_[0],index=DF_SVM_div.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Comparing between the Logistic Regression and the SVM, Logistic Regression seems to be more appropriate.\n",
    "\n",
    "LR accuracy: 0.668105349379\n",
    "\n",
    "[[  253 10243]\n",
    "\n",
    "[  292 20954]]\n",
    "\n",
    "SVM accuracy: 0.669428517422\n",
    "\n",
    "[[    0 10493]\n",
    "\n",
    " [    0 21249]]\n",
    " \n",
    "We can see that the SVM generated a confusion matrix with 21249 true positives and 10493 false positives. Suspiciously, there are 0 false negatives and 0 true negatives. \n",
    "\n",
    "Because our output was 0 for false negatives and 0 true negatives, this does not bode well for our ROC curve. Since the ROC curve is a threshold measure for the precision/recall. The ROC curve will not be able to compute since Precision/Recall are not calculatable because there would be a 0 in the denomenator.\n",
    "\n",
    "Looking at the scaled weights for rno_000001, we can see that statistically, in the LR model grade, is_male, and step_emp are the greatest influences of the diversity classifier. This is concerning since being male seems to be an indication for caucasian. This seems to indicate that if an employee is male, they are more likely to be caucasian. \n",
    "\n",
    "For the SVM model, the scaled weights show service, experience, and start year to be the important factors for determining the diversity classifier. Since all these variables are related to start date, it implies that the long-term employees that have been with NASA for an extended period are more likely to be caucasian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">10 Points - Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results for salary range jr level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Junior Salary Range Accuracy with 10 splits 0.831877574488\n",
      "[[71338 10352]\n",
      " [10994 34283]]\n",
      "--------------------------------------\n",
      "Random Forest Salary_Range_Jr_level Accuracy 0.902770011105\n",
      "[[72408  9282]\n",
      " [ 3063 42214]]\n",
      "--------------------------------------\n",
      "SVM Junior Salary Range accuracy: 0.861665931573\n",
      "[[18859  1420]\n",
      " [ 2971  8492]]\n"
     ]
    }
   ],
   "source": [
    "print ('KNN Junior Salary Range Accuracy with 10 splits', KNNs_total_accuracy)\n",
    "print(KNNs_conf)\n",
    "print('--------------------------------------')\n",
    "print ('Random Forest Salary_Range_Jr_level Accuracy', RFs_total_accuracy)\n",
    "print (RFs_conf)\n",
    "print('--------------------------------------')\n",
    "print('SVM Junior Salary Range accuracy:', SVMs_acc2)\n",
    "print(SVMs_conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ANOVA ANALYSIS\n",
    "#### Procedure\n",
    "1. Step 1: Run each test 10 times, record the accuracy and stick into a small dataset\n",
    "2. Step 2: Run a one-way ANOVA on those datasets for a difference in means\n",
    "3. Step 3: Get an F-statistic and a p-value\n",
    "4. Step 4: find out which model is actually different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Null Hypothesis:\n",
    "Ho: M1 = M2 = M3 = M4 = M5 = M6 = M7 = M8 (There is no difference between the mean accuracy for each model.)\n",
    "\n",
    "#### Alternative Hypothesis:\n",
    "At least one of the Mean accuracies is different from the others.\n",
    "\n",
    "#### Alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# USE THIS CODE AND EXAMIN THE POST HOC TEST\n",
    "Classifiers = [(0.783148, 0.931139, 0.859933, 0.816489, 0.648696, 0.778399, 0.670468, 0.669743),\n",
    "              (0.779826, 0.940063, 0.857349, 0.808928, 0.636031, 0.781842, 0.676422, 0.668955),\n",
    "              (0.781239, 0.935716, 0.858641, 0.818411, 0.632951, 0.789251, 0.667758, 0.665994),\n",
    "              (0.778679, 0.936382, 0.861949, 0.812236, 0.629342, 0.787903, 0.671444, 0.667002),\n",
    "              (0.778655, 0.940166, 0.860185, 0.811732, 0.626857, 0.785273, 0.672169, 0.666876),\n",
    "              (0.779134, 0.929951, 0.860405, 0.815922, 0.615054, 0.765331, 0.669869, 0.675351),\n",
    "              (0.781443, 0.935715, 0.860689, 0.818190, 0.618937, 0.786807, 0.668735, 0.665616),\n",
    "              (0.781521, 0.936812, 0.861760, 0.814851, 0.623837, 0.777982, 0.668420, 0.668735),\n",
    "              (0.779653, 0.951117, 0.859523, 0.806187, 0.638300, 0.781137, 0.669428, 0.668136),\n",
    "              (0.778991, 0.938644, 0.863493, 0.811826, 0.625345, 0.788020, 0.671917, 0.669523)] \n",
    "Labels = [\"S_KNN\", \"S_RandomForest\", \"S_SVM\", \"S_LogisticReg\",\"D_KNN\", \"D_RandomForest\", \"D_SVM\", \"D_LogisticReg\"]\n",
    "df = pd.DataFrame.from_records(Classifiers, columns = Labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=4091.3845401976214, pvalue=6.1908577297490784e-91)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "S_KNN = df[\"S_KNN\"]\n",
    "S_RF = df[\"S_RandomForest\"] \n",
    "S_SVM= df[\"S_SVM\"]\n",
    "S_LR = df[\"S_LogisticReg\"]\n",
    "D_KNN = df[\"D_KNN\"]\n",
    "D_RF = df[\"D_RandomForest\"]\n",
    "D_SVM = df[\"D_SVM\"]\n",
    "D_LR = df[\"D_LogisticReg\"]\n",
    "          \n",
    "# PERFORM ONE-WAY ANOVA\n",
    "\n",
    "stats.f_oneway(S_KNN, S_RF, S_SVM, S_LR, D_KNN, D_RF, D_SVM, D_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test Statistics:\n",
    "After doing a one-way ANOVA on our models, we received an:\n",
    "#### F-statistic = 4091.38 and a P-Value < 0.0001. \n",
    "#### Statistical Conclusion Pt 1\n",
    "We reject the Null Hypothesis that there is no difference between the mean accuracy for each model. There is statistically significant evidence to conclude that at least one of the models is different from the others. \n",
    "\n",
    "We will proceed with a Post-Hoc test to determine which model is different from the others and then conclude which model is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_KNN S_RandomForest\n",
      "Ttest_indResult(statistic=-82.584228542276364, pvalue=1.1266584512789438e-24)\n",
      "S_KNN S_SVM\n",
      "Ttest_indResult(statistic=-109.62690717927192, pvalue=6.9448256853810431e-27)\n",
      "S_KNN S_LogisticReg\n",
      "Ttest_indResult(statistic=-24.523360528067631, pvalue=2.7782616743820722e-15)\n",
      "S_KNN D_KNN\n",
      "Ttest_indResult(statistic=47.709953658696378, pvalue=2.0966084630043161e-20)\n",
      "S_KNN D_RandomForest\n",
      "Ttest_indResult(statistic=-0.84618115935814164, pvalue=0.40856064591542851)\n",
      "S_KNN D_SVM\n",
      "Ttest_indResult(statistic=117.8536142807954, pvalue=1.8912382714903535e-27)\n",
      "S_KNN D_LogisticReg\n",
      "Ttest_indResult(statistic=111.57118688446205, pvalue=5.0632010093716276e-27)\n",
      "S_RandomForest S_SVM\n",
      "Ttest_indResult(statistic=40.108683332135925, pvalue=4.6357321835186599e-19)\n",
      "S_RandomForest S_LogisticReg\n",
      "Ttest_indResult(statistic=55.462740202734373, pvalue=1.4191842459197638e-21)\n",
      "S_RandomForest D_KNN\n",
      "Ttest_indResult(statistic=84.963482922165724, pvalue=6.7654804535827509e-25)\n",
      "S_RandomForest D_RandomForest\n",
      "Ttest_indResult(statistic=53.094666544023845, pvalue=3.0986820866314283e-21)\n",
      "S_RandomForest D_SVM\n",
      "Ttest_indResult(statistic=132.919883011628, pvalue=2.1743393756560424e-28)\n",
      "S_RandomForest D_LogisticReg\n",
      "Ttest_indResult(statistic=131.73374640587639, pvalue=2.5546493699067802e-28)\n",
      "S_SVM S_LogisticReg\n",
      "Ttest_indResult(statistic=33.939168667211007, pvalue=9.0266568055892829e-18)\n",
      "S_SVM D_KNN\n",
      "Ttest_indResult(statistic=72.824999528723168, pvalue=1.0767065101840483e-23)\n",
      "S_SVM D_RandomForest\n",
      "Ttest_indResult(statistic=33.439351182970078, pvalue=1.174321340042045e-17)\n",
      "S_SVM D_SVM\n",
      "Ttest_indResult(statistic=196.00156317682678, pvalue=2.0105576745781598e-31)\n",
      "S_SVM D_LogisticReg\n",
      "Ttest_indResult(statistic=185.08410610830504, pvalue=5.6380990473359377e-31)\n",
      "S_LogisticReg D_KNN\n",
      "Ttest_indResult(statistic=54.592735545451831, pvalue=1.8833942117974208e-21)\n",
      "S_LogisticReg D_RandomForest\n",
      "Ttest_indResult(statistic=12.021100540814885, pvalue=4.9057536906440582e-10)\n",
      "S_LogisticReg D_SVM\n",
      "Ttest_indResult(statistic=95.412116797888203, pvalue=8.4242104722182568e-26)\n",
      "S_LogisticReg D_LogisticReg\n",
      "Ttest_indResult(statistic=93.967531946476115, pvalue=1.1080643479207908e-25)\n",
      "D_KNN D_RandomForest\n",
      "Ttest_indResult(statistic=-39.534907018186011, pvalue=5.9917749452143166e-19)\n",
      "D_KNN D_SVM\n",
      "Ttest_indResult(statistic=-12.766331612364768, pvalue=1.8511770420880874e-10)\n",
      "D_KNN D_LogisticReg\n",
      "Ttest_indResult(statistic=-12.044671771940573, pvalue=4.7533726720603129e-10)\n",
      "D_RandomForest D_SVM\n",
      "Ttest_indResult(statistic=46.318094020145139, pvalue=3.5578707120344991e-20)\n",
      "D_RandomForest D_LogisticReg\n",
      "Ttest_indResult(statistic=46.630594646842468, pvalue=3.1552805702669654e-20)\n",
      "D_SVM D_LogisticReg\n",
      "Ttest_indResult(statistic=1.7472211103681021, pvalue=0.097638225450188285)\n"
     ]
    }
   ],
   "source": [
    "model_pairs = []\n",
    "\n",
    "for model1 in range(7):\n",
    "    for model2  in range(model1+1,8):\n",
    "        model_pairs.append((Labels[model1], Labels[model2]))\n",
    "\n",
    "# Conduct t-test on each pair\n",
    "for model1, model2 in model_pairs: \n",
    "    print(model1, model2)\n",
    "    print(stats.ttest_ind(df[model1], \n",
    "                          df[model2]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "D_SVM & D_LogisticReg (SVM model and Logisitic Regression model for \"rno_000001\" (or the diversity variable)) was the only legitimate model pairing that was not statistically different from each other.\n",
    "\n",
    "Side Note:\n",
    "(S_KNN & D_RandomForest was also seen to not be statistically different above, however, these are 2 very different models on 2 different variables, so they are disqualified from inference.)\n",
    "\n",
    "### Conclusion:\n",
    "With the Random Forest Classifier having the highest accuracy compared to any other model, and being statistically significantly different, we choose this model as the best model for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">10 Points - Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesbe sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "# Import some data to play with\n",
    "\n",
    "DF_ROC = DF_Reg2.copy()\n",
    "iris = DF_ROC\n",
    "if 'salary_range_Jr_Level' in iris:\n",
    "    y = iris['salary_range_Jr_Level'].values \n",
    "    del iris['salary_range_Jr_Level']\n",
    "    X = iris.values\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.right](ROC_sal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the ROC curve, we can see that the SVM model visually confirms our performance of the binary classifier for Junior level salary range. The area under the curve shows a pronouced True Positive Rate. \n",
    "\n",
    "An ROC curve demonstrates several things:\n",
    "\n",
    "1. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n",
    "\n",
    "2. The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n",
    "\n",
    "3. The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n",
    "\n",
    "source: http://gim.unmc.edu/dxtests/roc2.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ROC CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "DF_ROC = DF_Reg2.copy()\n",
    "iris = DF_ROC\n",
    "\n",
    "if 'rno_000001' in iris:\n",
    "    y = iris['rno_000001'].values \n",
    "    del iris['rno_000001']\n",
    "    X = iris.values\n",
    "\n",
    "y = label_binarize(y, classes=[0, 1]) \n",
    "\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the ROC curve for Junior level salary range we can see the area under the curve has minimal false negatives and false positives for the KNN model. \n",
    "\n",
    "Looking at the ROC curve for the diversity classification we can see the area under the curve has minimal false negatives and false positives for the random forest model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Modeling and Evaluation 6\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">\n",
    "10 Points - Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT QUITE SURE WHAT CODE IS SUPPOSED TO GO HERE?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the SVM weights output, we know that for the Junior Level Salary Range, the most important variables for analysis were Grade, Service, Last Promoted Date, Step_Emp, and Retdiscdte are the biggest predictors of the Junior Salary Range Level classification. It is interesting to note that in the logistic regression model, 'grade' was not a relevant variable. However, in the SVM model we can see that 'grade' has the largest weight. \n",
    "\n",
    "We suspect that this is because Logistic Regression aims to fit the data points along a continuous function so the 'grade' variable passes into the model with minimal significance. However, for Support Vector Machines, it is easy to separate the data into classes since we can assume most junior salary range level employees are probably also defined by their 'grade' level. We initially defined the categories as follows.\n",
    "\n",
    "'Entry_Level' : $0-50,000\n",
    "'Jr_Level' : $50,000-100,000\n",
    "'Mid_Level' : $100,000-135,000\n",
    "'Mgmt_Level' : $135,000-150,000\n",
    "\n",
    "We know from NASA documentation that pay level is defined by a combination of 'grade', 'step', and 'service'. Knowing this, it makes sense that the biggest predictors of Junior Salary Range are 'grade', 'step' and 'service'. \n",
    "\n",
    "Looking at the SVM weights output for the diversity classifier, most of the variables had minimal effect on the diversity classification. However, 'Service', 'EODyr' and 'Experience' carried the largest amount of weight with Service weighting in at -0.002. We noticed that these variables are all time based. We suspect this may have something to do with their negative correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deployment\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<span style=\"color:red\">5 Points - How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All Federal Agencies are required to use and store the data elements defined by Office of Personnel Management (OPM). The data elements are defined in the following OPM website: https://www.opm.gov/policy-data-oversight/pay-leave/salaries-wages/salary-tables/datadictionary.aspx\n",
    "\n",
    "We strongly recommend all Human Resources and Workforce Planning offices of the United States Government, especially the Office of Personnel Management (OPM) will benefit from the use of the models and analysis contained in the document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exceptional Work\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Classification Using Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble  import GradientBoostingClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = DF_NoT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target = 'rno_000001' #Binary for being White or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train[target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define a function for modeling and cross-validation\n",
    "This function will do the following:\n",
    "1. fit the model\n",
    "2. determine training accuracy\n",
    "3. determine training AUC\n",
    "4. determine testing AUC\n",
    "5. perform CV is performCV is True\n",
    "6. plot Feature Importance if printFeatureImportance is True\n",
    "\n",
    "Source code from https://github.com/aarshayj/Analytics_Vidhya/blob/master/Articles/Parameter_Tuning_GBM_with_Example/GBM%20model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['rno_000001'])\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain['rno_000001'], cv=cv_folds, scoring='roc_auc')\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['rno_000001'].values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['rno_000001'], dtrain_predprob))\n",
    "    \n",
    "    if performCV:\n",
    "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "                \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except the target variable, in this case 'rno_000001'\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbm0 = GradientBoostingClassifier(random_state=10)\n",
    "modelfit(gbm0, train, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Describe what you see above ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GBM Models:\n",
    "There 2 types of parameters here:\n",
    "    \n",
    "Tree-specific parameters\n",
    "   \n",
    "2. min_samples_split\n",
    "3. min_samples_leaf\n",
    "4. max_depth\n",
    "5. min_leaf_nodes\n",
    "6. max_features\n",
    "7. loss function\n",
    "   \n",
    "Boosting specific paramters\n",
    "   \n",
    "1. n_estimators\n",
    "2. learning_rate\n",
    "3. subsample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tune the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "param_test1 = {'n_estimators':[600, 700, 800, 900, 1000]}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=1, min_samples_split=500,min_samples_leaf=50,max_depth=8, max_features='sqrt', subsample=0.8,random_state=10), \n",
    "param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "So we can see from above that we got 800 as the optimal estimators for the 1.2 learning rate. We started testing at a learning rate of 0.1 and the estimators were well above 1000. We used the following for learning rates and tested for optimal estimators:\n",
    "[0.1, 0.25, 0.5, 1.0, 1.2, 1.5, 2.0, 5.0]. After learning rate 1.5, the mean significantly drops to the mid 0.50's. We will proceed with 800 estimators and 1.2 learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "modelfit(gsearch1.best_estimator_, train, test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "param_test2 = {'max_depth':[2, 4, 6, 8, 10], 'min_samples_split':[100, 200, 300]}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=800,\n",
    "                                                max_features='sqrt', subsample=0.8, random_state=10), \n",
    "                       param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "10 Points - You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
